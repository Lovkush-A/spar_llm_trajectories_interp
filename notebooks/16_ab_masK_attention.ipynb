{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from taker import Model\n",
    "from taker.hooks import HookConfig\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Model(\"google/gemma-2-2b-it\", dtype=\"int8\")\n",
    "# m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../promptsV1.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    readdata = list(reader)\n",
    "    readdata = readdata[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 200\n",
    "temperature = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tell me about a weekend in a mountain cabin in 150 words and then tell me about disconnecting from technology in another 150 words. Only do that. Make sure you don’t add any headings or comments.\\n\\nThe scent of pine needles filled the air as we drove up the winding mountain road.  Our cozy cabin, nestled amongst towering trees, welcomed us with warmth and the promise of a peaceful escape.  Days were spent hiking through sun-dappled forests, the sound of birdsong our only soundtrack. Evenings were spent by the crackling fireplace, sharing stories and laughter. The stars, unfiltered by city lights, blazed across the night sky, a breathtaking spectacle.  \\n\\n'\n"
     ]
    }
   ],
   "source": [
    "orig_df = pd.read_json(f\"../gemma9b_results/latest_orig_generation_new.jsonl\", lines=True)\n",
    "def split_at_double_newline(text):\n",
    "    # Ensure we are only working with strings longer than 15 characters\n",
    "    if len(text) > 15:\n",
    "        # Search for the first double newline after the 15th character\n",
    "        pos = text.find('\\n\\n', 15)\n",
    "        if pos != -1:  # Check if double newline was found\n",
    "            return text[:pos+2], text[pos:]  # Split and remove the newline from the second part\n",
    "    return text, text  # If no split is required, return the original text and None\n",
    "\n",
    "orig_df['paragraph1'], orig_df['paragraph2'] = zip(*orig_df['output'].apply(split_at_double_newline))\n",
    "orig_df['paragraph1'] = orig_df['prompt'].astype(str) + orig_df['paragraph1'].astype(str)\n",
    "print(repr(orig_df['paragraph1'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "m = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is currently on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "m = m.to('cuda')\n",
    "device = m.device\n",
    "print(f\"Model is currently on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', 'Tell', 'Ġme', 'Ġabout', 'Ġa', 'Ġweekend', 'Ġin', 'Ġa', 'Ġmountain', 'Ġcabin', 'Ġin', 'Ġ', '150', 'Ġwords', 'Ġand', 'Ġthen', 'Ġtell', 'Ġme', 'Ġabout', 'Ġdisconnect', 'ing', 'Ġfrom', 'Ġtechnology', 'Ġin', 'Ġanother', 'Ġ', '150', 'Ġwords', '.', 'ĠOnly', 'Ġdo', 'Ġthat', '.', 'ĠMake', 'Ġsure', 'Ġyou', 'Ġdon', 'âĢĻt', 'Ġadd', 'Ġany', 'Ġheadings', 'Ġor', 'Ġcomments', '.ĊĊ', 'The', 'Ġscent', 'Ġof', 'Ġpine', 'Ġneedles', 'Ġfilled', 'Ġthe', 'Ġair', 'Ġas', 'Ġwe', 'Ġdrove', 'Ġup', 'Ġthe', 'Ġwinding', 'Ġmountain', 'Ġroad', '.', 'Ġ', 'ĠOur', 'Ġcozy', 'Ġcabin', ',', 'Ġnestled', 'Ġamongst', 'Ġtowering', 'Ġtrees', ',', 'Ġwelcomed', 'Ġus', 'Ġwith', 'Ġwarmth', 'Ġand', 'Ġthe', 'Ġpromise', 'Ġof', 'Ġa', 'Ġpeaceful', 'Ġescape', '.', 'Ġ', 'ĠDays', 'Ġwere', 'Ġspent', 'Ġhiking', 'Ġthrough', 'Ġsun', '-d', 'app', 'led', 'Ġforests', ',', 'Ġthe', 'Ġsound', 'Ġof', 'Ġbirds', 'ong', 'Ġour', 'Ġonly', 'Ġsoundtrack', '.', 'ĠEven', 'ings', 'Ġwere', 'Ġspent', 'Ġby', 'Ġthe', 'Ġcrack', 'ling', 'Ġfireplace', ',', 'Ġsharing', 'Ġstories', 'Ġand', 'Ġlaughter', '.', 'ĠThe', 'Ġstars', ',', 'Ġun', 'filtered', 'Ġby', 'Ġcity', 'Ġlights', ',', 'Ġbl', 'azed', 'Ġacross', 'Ġthe', 'Ġnight', 'Ġsky', ',', 'Ġa', 'Ġbreathtaking', 'Ġspectacle', '.', 'ĠĠĊĊ']\n",
      "torch.Size([1, 32, 140, 140])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3032\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streamer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3031\u001b[0m     streamer\u001b[38;5;241m.\u001b[39mput(next_tokens\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m-> 3032\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3033\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m unfinished_sequences \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mstopping_criteria(input_ids, scores)\n\u001b[1;32m   3039\u001b[0m this_peer_finished \u001b[38;5;241m=\u001b[39m unfinished_sequences\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:680\u001b[0m, in \u001b[0;36mGenerationMixin._update_model_kwargs_for_generation\u001b[0;34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m    679\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 680\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_ones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;66;03m# update decoder attention mask\u001b[39;00m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 2"
     ]
    }
   ],
   "source": [
    "# idlist = m.get_ids(orig_df['paragraph1'][0]).squeeze().tolist()\n",
    "# tokens = m.tokenizer.convert_ids_to_tokens(idlist)\n",
    "# print(tokens)\n",
    "def get_last_segment(text):\n",
    "    segments = text.split('\\n\\n')\n",
    "    return segments[-1].strip() if segments else ''\n",
    "\n",
    "generated_outputs = []\n",
    "for i, prompt in enumerate(orig_df['paragraph1'][:1]):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
    "    print(tokens)\n",
    "    input_length = len(tokens)\n",
    "    custom_attention_mask = torch.zeros(input_length,input_length)\n",
    "    custom_attention_mask[0:tokens.index('.ĊĊ'),0:tokens.index('.ĊĊ')] = 1\n",
    "    custom_attention_mask[len(tokens)-1,len(tokens)-1] = 1\n",
    "    attention_mask = custom_attention_mask.unsqueeze(0)\n",
    "    attention_mask = einops.repeat(attention_mask, 'm n o-> m k n o', k=32)\n",
    "    # inverted_attention_mask = 1 - attention_mask\n",
    "    # inverted_attention_mask[inverted_attention_mask == 1] = -float('inf')\n",
    "    # inputs['attention_mask'] = inverted_attention_mask\n",
    "    inputs['attention_mask'] = attention_mask\n",
    "\n",
    "    inputs['position_ids'] = torch.arange(input_length).unsqueeze(0)\n",
    "    print(inputs['attention_mask'].shape)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = m.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], position_ids = inputs['position_ids'],max_length=150, temperature=temperature,  do_sample=True)\n",
    "\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    generated_outputs.append(get_last_segment(generated_text))\n",
    "\n",
    "generated_df = pd.DataFrame({'generated_output': generated_outputs})\n",
    "print(generated_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still redoes the first paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world outside was forgotten, replaced by the quiet solitude of the mountain cabin. The only sounds were the wind, the crackling fire, and the gentle patter of the snow.  I spent\n"
     ]
    }
   ],
   "source": [
    "generated_df = pd.DataFrame({'generated_output': generated_outputs})\n",
    "print(generated_df['generated_output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# def get_last_segment(text):\n",
    "#     segments = text.split('\\n\\n')\n",
    "#     return '\\n\\n'.join(segments[2:]).strip() if len(segments) > 2 else ''\n",
    "# print(generated_df['generated_output'][0])\n",
    "# generated_df['generated_output'] = generated_df['generated_output'].apply(get_last_segment)\n",
    "# print(generated_df['generated_output'][0])\n",
    "with open(\"../gemma9b_results/latest_transferred_generation.jsonl\", 'r') as file:\n",
    "    transferred_outputs = [json.loads(line)['output'] for line in file][:20]\n",
    "transferred_df = pd.DataFrame({'transferred_output': transferred_outputs})\n",
    "combined_df = pd.concat([generated_df, transferred_df], axis=1)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "generated_embeddings = model.encode(combined_df['generated_output'].tolist())\n",
    "transferred_embeddings = model.encode(combined_df['transferred_output'].tolist())\n",
    "similarities = cosine_similarity(generated_embeddings, transferred_embeddings)\n",
    "combined_df['cosine_similarity'] = similarities.diagonal()\n",
    "\n",
    "# print(combined_df)\n",
    "# print(\"\\nAverage Cosine Similarity:\", combined_df['cosine_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      original_output generated_output  \\\n",
      "0   The world seemed to fade away as I stepped awa...                    \n",
      "1   The silence was profound, broken only by the r...                    \n",
      "2   The world fades away when you disconnect.  No ...                    \n",
      "3   The world outside faded away as I switched off...                    \n",
      "4   The silence was profound, broken only by the r...                    \n",
      "5   The world fades away when you disconnect from ...                    \n",
      "6   The silence was profound, broken only by the r...                    \n",
      "7   The world outside faded away as I switched off...                    \n",
      "8   The world outside faded away as I turned off m...                    \n",
      "9   The silence was profound, broken only by the r...                    \n",
      "10  The world seemed to shrink when I turned off m...                    \n",
      "11  The silence was profound, broken only by the r...                    \n",
      "12  The silence was profound, broken only by the r...                    \n",
      "13  The silence was profound, broken only by the r...                    \n",
      "14  The world seemed to fade away as I stepped awa...                    \n",
      "15  The silence was profound, broken only by the r...                    \n",
      "16  The silence was profound, broken only by the r...                    \n",
      "17  The world seemed to fade away as we stepped aw...                    \n",
      "18  The world fades away when you disconnect.  The...                    \n",
      "19  The world fades away when you disconnect from ...                    \n",
      "\n",
      "                                   transferred_output  \\\n",
      "0   The world is a symphony of sounds, each one a ...   \n",
      "1   The world is full of distractions, but true pe...   \n",
      "2   Unplugging from the constant hum of technology...   \n",
      "3   The world is a symphony of interconnected expe...   \n",
      "4   The world is a symphony of interconnected expe...   \n",
      "5   The world is a symphony of interconnected expe...   \n",
      "6   The world is a symphony of interconnected expe...   \n",
      "7   The world is a symphony of distractions. Our m...   \n",
      "8   The world felt different, somehow. The air was...   \n",
      "9   The world is a symphony of interconnected expe...   \n",
      "10  The world is a symphony of sounds, each note a...   \n",
      "11  The digital world is a vast and ever-changing ...   \n",
      "12  The world is a symphony of sounds, each note c...   \n",
      "13  The world can be a noisy place. It's easy to g...   \n",
      "14  The world feels different when you're disconne...   \n",
      "15  The world moved at a different pace then. Ther...   \n",
      "16  The world is a symphony of sounds, each note a...   \n",
      "17  The world is a noisy place, full of distractio...   \n",
      "18  The world is a symphony of noise, a cacophony ...   \n",
      "19  The world is a symphony of interconnected expe...   \n",
      "\n",
      "    original_vs_transferred_similarity  original_vs_generated_similarity  \n",
      "0                             0.573436                          0.116033  \n",
      "1                             0.479712                          0.134338  \n",
      "2                             0.803838                          0.068978  \n",
      "3                             0.513314                          0.135466  \n",
      "4                             0.446286                          0.128327  \n",
      "5                             0.451062                          0.099366  \n",
      "6                             0.565792                          0.128436  \n",
      "7                             0.550982                          0.105769  \n",
      "8                             0.557676                          0.118989  \n",
      "9                             0.415936                          0.114898  \n",
      "10                            0.293796                          0.099088  \n",
      "11                            0.204727                          0.155497  \n",
      "12                            0.554567                          0.086582  \n",
      "13                            0.356994                          0.104770  \n",
      "14                            0.668615                          0.136871  \n",
      "15                            0.612217                          0.146948  \n",
      "16                            0.404643                          0.150019  \n",
      "17                            0.599263                          0.147147  \n",
      "18                            0.510300                          0.133466  \n",
      "19                            0.494519                          0.120938  \n",
      "\n",
      "Average Cosine Similarity (Original vs Transferred): 0.5028838\n",
      "Average Cosine Similarity (Original vs Generated): 0.1215963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read original outputs\n",
    "with open(\"../gemma9b_results/latest_orig_generation_new.jsonl\", 'r') as file:\n",
    "    original_outputs = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= 20:\n",
    "            break\n",
    "        text = json.loads(line)['output']\n",
    "        segments = text.split('\\n\\n')\n",
    "        if len(segments) > 2:\n",
    "            original_outputs.append('\\n\\n'.join(segments[2:]).strip())\n",
    "        else:\n",
    "            original_outputs.append('')\n",
    "\n",
    "original_df = pd.DataFrame({'original_output': original_outputs})\n",
    "combined_df = pd.concat([original_df, generated_df, transferred_df], axis=1)\n",
    "original_embeddings = model.encode(combined_df['original_output'].tolist())\n",
    "\n",
    "original_vs_transferred = cosine_similarity(original_embeddings, transferred_embeddings)\n",
    "original_vs_generated = cosine_similarity(original_embeddings, generated_embeddings)\n",
    "\n",
    "combined_df['original_vs_transferred_similarity'] = original_vs_transferred.diagonal()\n",
    "combined_df['original_vs_generated_similarity'] = original_vs_generated.diagonal()\n",
    "\n",
    "print(combined_df)\n",
    "print(\"\\nAverage Cosine Similarity (Original vs Transferred):\", combined_df['original_vs_transferred_similarity'].mean())\n",
    "print(\"Average Cosine Similarity (Original vs Generated):\", combined_df['original_vs_generated_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1378058850.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# outputs = m.generate(**inputs, max_new_tokens=150, temperature=temperature, do_sample=True)\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# # outputs = m.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=50, temperature=temperature)\n",
    "# # generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# # The error suggests that the model doesn't have a language model head for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral_prompts = [\"\\n\\n\"]\n",
    "# # Apply the function to the DataFrame column\n",
    "# filename = f\"../gemma9b_results/latest_transferred_generation.jsonl\"\n",
    "# if not exists(filename):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         pass\n",
    "    \n",
    "# for info_prompt in orig_df['paragraph1']:\n",
    "#     acts = m.get_midlayer_activations(info_prompt)\n",
    "#     orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "#     for neutral_prompt in neutral_prompts:\n",
    "#         new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1        \n",
    "\n",
    "#         [h.reset() for h in m.hooks.neuron_replace.values()] #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "#         for layer_index in range(0,42):\n",
    "#             m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, acts[\"mlp\"][0, layer_index, orig_token_index])\n",
    "#             m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, acts[\"attn\"][0, layer_index, orig_token_index])\n",
    "#         with HiddenPrints():\n",
    "#             # for i in range(1):\n",
    "#                 output = m.generate(neutral_prompt, max_new_tokens, temperature=temperature)\n",
    "                \n",
    "#                 data = {\n",
    "#                     \"temperature\": temperature,\n",
    "#                     \"max_new_tokens\": max_new_tokens,\n",
    "#                     \"model\": \"google/gemma-2-9b-it\",\n",
    "#                     \"type\": \"transferred\",\n",
    "#                     \"num_transferred_tokens\": 1,\n",
    "#                     \"transplant_layers\": (0,42),\n",
    "#                     \"orig_prompt\": info_prompt,\n",
    "#                     \"transplant_prompt\": neutral_prompt,\n",
    "#                     \"output\": output[1],\n",
    "#                 }\n",
    "\n",
    "#                 with open(filename, \"a\") as file:\n",
    "#                     file.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info_prompt in orig_df['paragraph1']:\n",
    "    acts = m.get_midlayer_activations(info_prompt)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
