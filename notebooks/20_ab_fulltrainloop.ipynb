{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import circuitsvis as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.decomposition import PCA\n",
    "from taker import Model\n",
    "from taker.hooks import HookConfig\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "import einops\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# m = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'meta-llama/Llama-3.2-1B-Instruct' with bfp16:\n",
      "- Added 256 hooks across 16 layers\n",
      "Loaded model 'meta-llama/Llama-3.2-1B-Instruct' with bfp16:\n",
      "- Added 256 hooks across 16 layers\n",
      " - n_layers : 16\n",
      " - d_model  : 2048\n",
      " - n_heads  : 32\n",
      " - d_head   : 64\n",
      " - d_mlp    : 8192\n",
      "Initialized PEFT model\n",
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model_kwargs = {\"attn_implementation\": \"eager\"}\n",
    "m = Model(model_repo=\"meta-llama/Llama-3.2-1B-Instruct\", model_kwargs=model_kwargs)\n",
    "m_orig = Model(model_repo=\"meta-llama/Llama-3.2-1B-Instruct\", model_kwargs=model_kwargs)\n",
    "# m = Model(model_repo=\"nickypro/tinyllama-15m\")\n",
    "# m_orig = Model(model_repo=\"nickypro/tinyllama-15m\")\n",
    "m.show_details()\n",
    "tokenizer = m.tokenizer\n",
    "predictor = m.predictor\n",
    "\n",
    "# Initialize PEFT\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "m.init_peft(peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - n_layers : 16\n",
      " - d_model  : 2048\n",
      " - n_heads  : 32\n",
      " - d_head   : 64\n",
      " - d_mlp    : 8192\n"
     ]
    }
   ],
   "source": [
    "m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../promptsV1.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    readdata = list(reader)\n",
    "    readdata = readdata[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 200\n",
    "temperature = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tell me about a weekend in a mountain cabin in 150 words and then tell me about disconnecting from technology in another 150 words. Only do that. Make sure you donâ€™t add any headings or comments.\\n\\nThe scent of pine needles filled the air as we drove up the winding mountain road.  Our cozy cabin, nestled amongst towering trees, welcomed us with warmth and the promise of a peaceful escape.  Days were spent hiking through sun-dappled forests, the sound of birdsong our only soundtrack. Evenings were spent by the crackling fireplace, sharing stories and laughter. The stars, unfiltered by city lights, blazed across the night sky, a breathtaking spectacle.  \\n\\n'\n"
     ]
    }
   ],
   "source": [
    "orig_df = pd.read_json(f\"../gemma9b_results/latest_orig_generation_new.jsonl\", lines=True)\n",
    "def split_at_double_newline(text):\n",
    "    # Ensure we are only working with strings longer than 15 characters\n",
    "    if len(text) > 15:\n",
    "        # Search for the first double newline after the 15th character\n",
    "        pos = text.find('\\n\\n', 15)\n",
    "        if pos != -1:  # Check if double newline was found\n",
    "            return text[:pos+2], text[pos:]  # Split and remove the newline from the second part\n",
    "    return text, text  # If no split is required, return the original text and None\n",
    "\n",
    "orig_df['paragraph1'], orig_df['paragraph2'] = zip(*orig_df['output'].apply(split_at_double_newline))\n",
    "orig_df['paragraph1'] = orig_df['prompt'].astype(str) + orig_df['paragraph1'].astype(str)\n",
    "print(repr(orig_df['paragraph1'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = m.to('cuda')\n",
    "device = m.device\n",
    "# print(f\"Model is currently on: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_paragraph_attention_mask(input_ids, split_id, device=\"cuda\"):\n",
    "#     # Ensure input_ids is 2D\n",
    "#     if len(input_ids.shape) == 1:\n",
    "#         input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "#     batch_size, seq_length = input_ids.shape\n",
    "\n",
    "#     # Create base causal mask (lower triangular)\n",
    "#     causal_mask = torch.ones((batch_size, 1, seq_length, seq_length), dtype=torch.bool, device=device)\n",
    "#     for i in range(seq_length):\n",
    "#         for j in range(i + 1, seq_length):\n",
    "#             causal_mask[:, :, i, j] = False\n",
    "\n",
    "#     # Find dot positions\n",
    "#     dot_indices = []\n",
    "\n",
    "#     for b in range(batch_size):\n",
    "#         for i in range(seq_length):\n",
    "#             if input_ids[b, i] == split_id:\n",
    "#                 dot_indices.append(i)\n",
    "\n",
    "#     # Modify mask for paragraph attention control:\n",
    "#     # 1. Block attention between paragraphs\n",
    "#     # 2. Allow dots to see previous paragraphs\n",
    "#     # 3. Allow next paragraph to see the dot\n",
    "#     for dot_idx in dot_indices:\n",
    "#         # Block attention from tokens after dot to tokens before dot\n",
    "#         causal_mask[:, :, (dot_idx+1):, :dot_idx] = False\n",
    "\n",
    "#         # Allow dot to see all previous tokens\n",
    "#         causal_mask[:, :, dot_idx, :dot_idx] = True\n",
    "\n",
    "#         # Allow tokens after dot to see the dot\n",
    "#         causal_mask[:, :, (dot_idx+1):, dot_idx] = True\n",
    "\n",
    "#     # Convert to float and set -inf for masked positions\n",
    "#     attention_mask = causal_mask.float()\n",
    "#     # attention_mask = attention_mask.masked_fill(~causal_mask, float(\"-inf\"))\n",
    "\n",
    "#     return attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraph_attention_mask(input_ids, split_id, device=\"cuda\"): #modify the mask to do the\n",
    "    # Ensure input_ids is 2D\n",
    "    if len(input_ids.shape) == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    batch_size, seq_length = input_ids.shape\n",
    "\n",
    "    # Create base causal mask (lower triangular)\n",
    "    causal_mask = torch.ones((batch_size, 1, seq_length, seq_length), dtype=torch.bool, device=device)\n",
    "    for i in range(seq_length):\n",
    "        for j in range(i + 1, seq_length):\n",
    "            causal_mask[:, :, i, j] = False\n",
    "\n",
    "    # Find dot positions\n",
    "    dot_idx = None\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for i in range(seq_length):\n",
    "            if input_ids[b, i] == split_id:\n",
    "                dot_idx = i\n",
    "                break\n",
    "        if dot_idx:\n",
    "            break\n",
    "\n",
    "    # Modify mask for paragraph attention control:\n",
    "    # 1. Block attention between paragraphs\n",
    "    # 2. Allow dots to see previous paragraphs\n",
    "    # 3. Allow next paragraph to see the dot\n",
    "\n",
    "    causal_mask[:, :, (dot_idx):, :dot_idx] = False\n",
    "\n",
    "    # Allow dot to see all previous tokens\n",
    "    causal_mask[:, :, dot_idx, :dot_idx] = True\n",
    "\n",
    "    # Allow tokens after dot to see the dot\n",
    "    # causal_mask[:, :, (dot_idx+1):, dot_idx] = True\n",
    "    causal_mask[:, :, (dot_idx):, dot_idx] = True\n",
    "\n",
    "\n",
    "    # Convert to float and set -inf for masked positions\n",
    "    attention_mask = causal_mask.float()\n",
    "    attention_mask = attention_mask.masked_fill(~causal_mask, float(\"-inf\"))\n",
    "    # attention_mask[~np.isinf(attention_mask)] = 0\n",
    "\n",
    "\n",
    "    return attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraph_attention_mask(input_ids, split_id=None, split_index=None, device=\"cuda\"):\n",
    "    # Ensure input_ids is 2D [batch, n_tokens]\n",
    "    if len(input_ids.shape) == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    batch_size, seq_length = input_ids.shape\n",
    "\n",
    "    # Create base causal mask (lower triangular)\n",
    "    causal_mask = torch.ones((batch_size, 1, seq_length, seq_length), dtype=torch.bool, device=device)\n",
    "    for i in range(seq_length):\n",
    "        for j in range(i + 1, seq_length):\n",
    "            causal_mask[:, :, i, j] = False\n",
    "\n",
    "    # Find dot position (first occurrence)\n",
    "    dot_idx = None\n",
    "    for b in range(batch_size):\n",
    "        if split_index is not None:\n",
    "            dot_idx = split_index\n",
    "            break\n",
    "        for i in range(seq_length):\n",
    "            if input_ids[b, i] == split_id:\n",
    "                dot_idx = i\n",
    "                break\n",
    "        if dot_idx:\n",
    "            break\n",
    "\n",
    "    if dot_idx:\n",
    "        # Block attention between paragraphs\n",
    "        causal_mask[:, :, (dot_idx):, :dot_idx] = False\n",
    "        # Allow dot to see all previous tokens\n",
    "        causal_mask[:, :, dot_idx, :dot_idx] = True\n",
    "        # Allow tokens after dot to see the dot\n",
    "        causal_mask[:, :, (dot_idx):, dot_idx] = True\n",
    "\n",
    "    # Convert to float\n",
    "    attention_mask = causal_mask\n",
    "\n",
    "    # turn \"true\" into 0 and \"false\" into -inf\n",
    "    attention_mask = torch.where(attention_mask, torch.tensor(0.0), torch.tensor(-np.inf))\n",
    "    return attention_mask\n",
    "\n",
    "create_paragraph_attention_mask_default = lambda inputs: create_paragraph_attention_mask(inputs, split_id=m.tokenizer.encode(\".\\n\\n\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_activations(model, text, attention_mask = None):\n",
    "        model.hooks.disable_all_collect_hooks()\n",
    "        model.hooks.enable_collect_hooks([\"mlp_pre_out\", \"attn_pre_out\"])\n",
    "\n",
    "        # Run model\n",
    "        if model.tokenizer.pad_token is None:\n",
    "            model.tokenizer.pad_token = model.tokenizer.eos_token\n",
    "        # print(text)\n",
    "        inputs = model.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs.input_ids\n",
    "        logits = model.get_logits(input_ids=input_ids, attention_mask=attention_mask)        # Collect and return activaitons\n",
    "        acts = {\n",
    "            \"attn\": model.collect_recent_attn_pre_out(),\n",
    "            \"mlp\":  model.collect_recent_mlp_pre_out(),\n",
    "        }\n",
    "        return acts, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt(prompt, target_len, tokenizer, predictor):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
    "        # print(tokens)\n",
    "        input_length = len(tokens)\n",
    "\n",
    "        dot_id = tokenizer.encode(\".\\n\\n\")[-1]\n",
    "        # print(f\"dot_id: {dot_id}, token: {tokenizer.convert_ids_to_tokens(dot_id)}\")\n",
    "        inputs['attention_mask'] = create_paragraph_attention_mask(inputs['input_ids'], dot_id, device=device)\n",
    "\n",
    "        import seaborn as sns\n",
    "\n",
    "        sns.heatmap(inputs['attention_mask'][0, 0].cpu().numpy())\n",
    "        # print(inputs['attention_mask'].shape)\n",
    "\n",
    "        # repeat attention mask\n",
    "        inputs['attention_mask'] = einops.repeat(inputs['attention_mask'], 'batch 1 i j-> batch head i j', head=m.cfg.n_heads)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to(torch.bfloat16)\n",
    "\n",
    "        inputs['position_ids'] = torch.arange(input_length).unsqueeze(0)\n",
    "        # print(inputs['attention_mask'].shape)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = predictor(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], position_ids=inputs['position_ids'])\n",
    "        # print(\"logits: \", outputs['logits'].shape)\n",
    "\n",
    "        decoded_texts = []\n",
    "        predicted_token_id = torch.argmax(outputs['logits'][:, -1, :])\n",
    "        predicted_text = tokenizer.decode(predicted_token_id)\n",
    "        decoded_texts.append(predicted_text)\n",
    "\n",
    "        while input_length < target_len:  # make it the same length as the ground truth\n",
    "            predicted_token_id = predicted_token_id.reshape(1, 1)\n",
    "            inputs['input_ids'] = torch.cat((inputs['input_ids'], predicted_token_id), dim=1).to('cuda')\n",
    "            inputs['attention_mask'] = create_paragraph_attention_mask(inputs['input_ids'], dot_id, device=device).to('cuda')\n",
    "            inputs['attention_mask'] = inputs['attention_mask'].to(torch.bfloat16)\n",
    "\n",
    "            input_length += 1\n",
    "            inputs['position_ids'] = torch.arange(input_length).unsqueeze(0).to('cuda')\n",
    "            outputs = predictor(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], position_ids=inputs['position_ids'])\n",
    "            predicted_token_id = torch.argmax(outputs['logits'][:, -1, :])\n",
    "            predicted_text = tokenizer.decode(predicted_token_id)\n",
    "            decoded_texts.append(predicted_text)\n",
    "\n",
    "        # print(''.join(repr(decoded_texts)))\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to(float)\n",
    "        idlist = [tokenizer.decode(x) for x in inputs['input_ids']]\n",
    "        # print(idlist[0])\n",
    "        # sns.heatmap(inputs['attention_mask'][0, 0].cpu().numpy())\n",
    "        return idlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 32\n"
     ]
    }
   ],
   "source": [
    "print(m.cfg.n_layers, m.cfg.n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about a weekend in a mountain cabin in 150 words and then tell me about disconnecting from technology in another 150 words. Only do that. Make sure you donâ€™t add any headings or comments.\n",
      "\n",
      "The scent of pine needles filled the air as we drove up the winding mountain road.  Our cozy cabin, nestled amongst towering trees, welcomed us with warmth and the promise of a peaceful escape.  Days were spent hiking through sun-dappled forests, the sound of birdsong our only soundtrack. Evenings were spent by the crackling fireplace, sharing stories and laughter. The stars, unfiltered by city lights, blazed across the night sky, a breathtaking spectacle.  \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# idlist = m.get_ids(orig_df['paragraph1'][0]).squeeze().tolist()\n",
    "# tokens = m.tokenizer.convert_ids_to_tokens(idlist)\n",
    "# print(tokens)\n",
    "def get_last_segment(text):\n",
    "    segments = text.split('.\\n\\n')\n",
    "    return segments[-1].strip() if segments else ''\n",
    "\n",
    "generated_outputs = []\n",
    "for i, prompt in enumerate(orig_df['paragraph1'][:1]):\n",
    "    print(prompt)\n",
    "    process_prompt(prompt, target_len=100, tokenizer=m.tokenizer, predictor=m.predictor)\n",
    "    # prompt = \"Write a paragraph about dogs.\\n\\n\"\n",
    "\n",
    "    # predicted_token_id = torch.argmax(outputs['logits'][:, -1, :], axis=-1)\n",
    "    # predicted_text = tokenizer.decode(predicted_token_id)\n",
    "    # print(predicted_text)\n",
    "    # generated_text = tokenizer.decode(outputs['logits'], skip_special_tokens=True)\n",
    "    # print(generated_text)\n",
    "    # generated_outputs.append(get_last_segment(generated_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still redoes the first paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df['full'] = orig_df['prompt'].astype(str) + orig_df['output'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN/TEST SPLIT of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_split_column(df, column_name, test_size=0.1, random_state=None):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Extract the specified column\n",
    "    column_data = df[column_name]\n",
    "    print(\"df length: \", str(df.shape[0]))\n",
    "    # Perform train-test split\n",
    "    train_set, test_set = train_test_split(column_data, test_size=test_size, random_state=random_state)\n",
    "    print(\"train set length: \", str(train_set.shape[0]))\n",
    "    print(\"test set length: \", str(test_set.shape[0]))\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df length:  1000\n",
      "train set length:  900\n",
      "test set length:  100\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split_column(orig_df, \"full\", test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521    Tell me about parkour in 150 words and then te...\n",
      "737    Tell me about historical re-enactment in 150 w...\n",
      "740    Tell me about historical re-enactment in 150 w...\n",
      "660    Tell me about still-life drawing in 150 words ...\n",
      "411    Tell me about lucid dreaming in 150 words and ...\n",
      "                             ...                        \n",
      "436    Tell me about lucid dreaming in 150 words and ...\n",
      "764    Tell me about World War II in 150 words and th...\n",
      "88     Tell me about a road trip in 150 words and the...\n",
      "63     Tell me about a road trip in 150 words and the...\n",
      "826    Tell me about cleaning your house in 150 words...\n",
      "Name: full, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_peft_model(model, num_epochs=5, learning_rate=1e-3):\n",
    "    # Prepare the training data\n",
    "    # input_text = \"Generate the letter a: \"\n",
    "    # target_text = \"a \" * 20  # 20 'a' tokens\n",
    "    # full_text = input_text + target_text\n",
    "    # gt_text = \"Tell me about a weekend in a mountain cabin in 150 words and then tell me about disconnecting from technology in another 150 words. Only do that. Make sure you donâ€™t add any headings or comments.\\n\\nThe scent of pine needles filled the air as we drove up the winding mountain road.  Our cozy cabin, nestled amongst towering trees, welcomed us with warmth and the promise of a peaceful escape.  Days were spent hiking through sun-dappled forests, the sound of birdsong our only soundtrack. Evenings were spent by the crackling fireplace, sharing stories and laughter. The stars, unfiltered by city lights, blazed across the night sky, a breathtaking spectacle.\\n\\nThe world seemed to fade away as I stepped away from the constant buzz of notifications and the endless scroll.  Silence, once a rarity, became a comforting presence.  I rediscovered the joy of reading a physical book, the pages turning slowly, each word savored.  Conversations flowed more deeply, unhurried and genuine.  Nature, with its gentle rhythms, became my guide, reminding me of the simple beauty of being present.\"\n",
    "    # prompt = \"Tell me about a weekend in a mountain cabin in 150 words and then tell me about disconnecting from technology in another 150 words. Only do that. Make sure you donâ€™t add any headings or comments.\\n\\nThe scent of pine needles filled the air as we drove up the winding mountain road.  Our cozy cabin, nestled amongst towering trees, welcomed us with warmth and the promise of a peaceful escape.  Days were spent hiking through sun-dappled forests, the sound of birdsong our only soundtrack. Evenings were spent by the crackling fireplace, sharing stories and laughter. The stars, unfiltered by city lights, blazed across the night sky, a breathtaking spectacle.\\n\\n\"\n",
    "    tokenizer = model.tokenizer\n",
    "\n",
    "\n",
    "    # Prepare optimizer and scheduler\n",
    "    optimizer = AdamW(model.peft_predictor.parameters(), lr=learning_rate)\n",
    "    total_steps = num_epochs * 10  # 10 steps per epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    # Training loop\n",
    "    model.peft_predictor.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch = 0\n",
    "        for gt_text in train:  # 10 steps per epoch\n",
    "            # outputs = model.peft_predictor(\n",
    "            #     input_ids=input_ids,\n",
    "            #     attention_mask=attention_mask,\n",
    "            #     labels=labels\n",
    "            # )\n",
    "            # ------------- predictor loop insertion --------\n",
    "            gt_len = len(tokenizer.tokenize(gt_text))\n",
    "            inputs = tokenizer(gt_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "            dot_id = tokenizer.encode(\".\\n\\n\")[-1]\n",
    "            # print(f\"dot_id: {dot_id}, token: {tokenizer.convert_ids_to_tokens(dot_id)}\")\n",
    "            inputs['attention_mask'] = create_paragraph_attention_mask(inputs['input_ids'], dot_id, device=device)\n",
    "            # print({\"input_ids shape\": inputs['input_ids'].shape, \"attention_mask shape\": inputs['attention_mask'].shape})\n",
    "\n",
    "            # import seaborn as sns\n",
    "\n",
    "            # sns.heatmap(inputs['attention_mask'][0, 0].cpu().numpy())\n",
    "            # print(inputs['attention_mask'].shape)\n",
    "\n",
    "            # repeat attention mask\n",
    "            inputs['attention_mask'] = einops.repeat(inputs['attention_mask'], 'batch 1 i j-> batch head i j', head=m.cfg.n_heads)\n",
    "\n",
    "\n",
    "            # -----------------------------------------------\n",
    "            with torch.no_grad():  # Wrap updates in no_grad\n",
    "                acts_orig, logits_orig = get_model_activations(m_orig, gt_text)\n",
    "                # replace_acts(model, text)\n",
    "            acts_lora, logits_lora = get_model_activations(model, gt_text, attention_mask = inputs['attention_mask'])\n",
    "            # acts_lora, logits_lora = get_model_activations(model, gt_text)\n",
    "\n",
    "            logprobs_logits_lora = torch.nn.functional.log_softmax(logits_lora, dim=-1)\n",
    "            probs_orig = torch.nn.functional.softmax(logits_orig, dim=-1)\n",
    "                # Calculate loss\n",
    "                # Calculate loss\n",
    "            # print/compare logits and activations to one another first\n",
    "            loss_acts = torch.nn.MSELoss()(acts_lora['attn'],acts_orig['attn'])\n",
    "            loss_logits = torch.nn.CrossEntropyLoss()(logprobs_logits_lora,probs_orig)\n",
    "            loss = loss_acts + loss_logits\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch+1}/{train.shape[0]}, Acts Loss: {loss_acts.item():.4f}, Logits Loss: {loss_logits.item():.4f}\")\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()  # Update parameters without tracking gradients\n",
    "            # optimizer.zero_grad()  # Clear gradients for the next step\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch+=1\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        #validation loop\n",
    "        total_act_loss = 0\n",
    "        total_logit_loss = 0\n",
    "        for gt_text in test:\n",
    "            gt_len = len(tokenizer.tokenize(gt_text))\n",
    "            inputs = tokenizer(gt_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "            dot_id = tokenizer.encode(\".\\n\\n\")[-1]\n",
    "            # print(f\"dot_id: {dot_id}, token: {tokenizer.convert_ids_to_tokens(dot_id)}\")\n",
    "            inputs['attention_mask'] = create_paragraph_attention_mask(inputs['input_ids'], dot_id, device=device)\n",
    "            # print({\"input_ids shape\": inputs['input_ids'].shape, \"attention_mask shape\": inputs['attention_mask'].shape})\n",
    "\n",
    "            with torch.no_grad():  # Wrap updates in no_grad\n",
    "\n",
    "                acts_orig, logits_orig = get_model_activations(m_orig, gt_text)\n",
    "                            # replace_acts(model, text)\n",
    "                acts_lora, logits_lora = get_model_activations(m, gt_text, attention_mask = inputs['attention_mask'])\n",
    "                        # acts_lora, logits_lora = get_model_activations(model, gt_text)\n",
    "            logprobs_logits_lora = torch.nn.functional.log_softmax(logits_lora, dim=-1)\n",
    "            probs_orig = torch.nn.functional.softmax(logits_orig, dim=-1)\n",
    "                # Calculate loss\n",
    "                # Calculate loss\n",
    "            # print/compare logits and activations to one another first\n",
    "            loss_acts = torch.nn.MSELoss()(acts_lora['attn'],acts_orig['attn'])\n",
    "            loss_logits = torch.nn.CrossEntropyLoss()(logprobs_logits_lora,probs_orig)\n",
    "            loss = loss_acts + loss_logits\n",
    "            total_act_loss+=loss_acts\n",
    "            total_logit_loss+=loss_logits\n",
    "        print(f\"Validation Epoch {epoch+1}/{num_epochs}, Average Acts Loss: {total_act_loss/test.shape[0]:.4f}, Average Logits Loss: {total_logit_loss/test.shape[0]:.4f}\")\n",
    "\n",
    "    model.peft_predictor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 1/900, Acts Loss: 0.0374, Logits Loss: 0.0068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Batch 2/900, Acts Loss: 0.0369, Logits Loss: 0.0072\n",
      "Epoch 1/1, Batch 3/900, Acts Loss: 0.0334, Logits Loss: 0.0046\n",
      "Epoch 1/1, Batch 4/900, Acts Loss: 0.0330, Logits Loss: 0.0045\n",
      "Epoch 1/1, Batch 5/900, Acts Loss: 0.0315, Logits Loss: 0.0051\n",
      "Epoch 1/1, Batch 6/900, Acts Loss: 0.0305, Logits Loss: 0.0047\n",
      "Epoch 1/1, Batch 7/900, Acts Loss: 0.0305, Logits Loss: 0.0035\n",
      "Epoch 1/1, Batch 8/900, Acts Loss: 0.0297, Logits Loss: 0.0045\n",
      "Epoch 1/1, Batch 9/900, Acts Loss: 0.0271, Logits Loss: 0.0045\n",
      "Epoch 1/1, Batch 10/900, Acts Loss: 0.0266, Logits Loss: 0.0035\n",
      "Epoch 1/1, Batch 11/900, Acts Loss: 0.0151, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 12/900, Acts Loss: 0.0128, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 13/900, Acts Loss: 0.0087, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 14/900, Acts Loss: 0.0107, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 15/900, Acts Loss: 0.0120, Logits Loss: 0.0036\n",
      "Epoch 1/1, Batch 16/900, Acts Loss: 0.0115, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 17/900, Acts Loss: 0.0129, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 18/900, Acts Loss: 0.0080, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 19/900, Acts Loss: 0.0106, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 20/900, Acts Loss: 0.0126, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 21/900, Acts Loss: 0.0103, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 22/900, Acts Loss: 0.0110, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 23/900, Acts Loss: 0.0104, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 24/900, Acts Loss: 0.0118, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 25/900, Acts Loss: 0.0110, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 26/900, Acts Loss: 0.0112, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 27/900, Acts Loss: 0.0093, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 28/900, Acts Loss: 0.0084, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 29/900, Acts Loss: 0.0081, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 30/900, Acts Loss: 0.0089, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 31/900, Acts Loss: 0.0086, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 32/900, Acts Loss: 0.0084, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 33/900, Acts Loss: 0.0074, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 34/900, Acts Loss: 0.0073, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 35/900, Acts Loss: 0.0062, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 36/900, Acts Loss: 0.0079, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 37/900, Acts Loss: 0.0067, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 38/900, Acts Loss: 0.0067, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 39/900, Acts Loss: 0.0078, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 40/900, Acts Loss: 0.0056, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 41/900, Acts Loss: 0.0071, Logits Loss: 0.0035\n",
      "Epoch 1/1, Batch 42/900, Acts Loss: 0.0062, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 43/900, Acts Loss: 0.0066, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 44/900, Acts Loss: 0.0066, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 45/900, Acts Loss: 0.0056, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 46/900, Acts Loss: 0.0054, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 47/900, Acts Loss: 0.0040, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 48/900, Acts Loss: 0.0055, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 49/900, Acts Loss: 0.0073, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 50/900, Acts Loss: 0.0060, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 51/900, Acts Loss: 0.0045, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 52/900, Acts Loss: 0.0081, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 53/900, Acts Loss: 0.0066, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 54/900, Acts Loss: 0.0062, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 55/900, Acts Loss: 0.0051, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 56/900, Acts Loss: 0.0074, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 57/900, Acts Loss: 0.0036, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 58/900, Acts Loss: 0.0054, Logits Loss: 0.0035\n",
      "Epoch 1/1, Batch 59/900, Acts Loss: 0.0062, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 60/900, Acts Loss: 0.0051, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 61/900, Acts Loss: 0.0049, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 62/900, Acts Loss: 0.0037, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 63/900, Acts Loss: 0.0051, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 64/900, Acts Loss: 0.0051, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 65/900, Acts Loss: 0.0047, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 66/900, Acts Loss: 0.0049, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 67/900, Acts Loss: 0.0064, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 68/900, Acts Loss: 0.0046, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 69/900, Acts Loss: 0.0056, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 70/900, Acts Loss: 0.0043, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 71/900, Acts Loss: 0.0053, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 72/900, Acts Loss: 0.0062, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 73/900, Acts Loss: 0.0051, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 74/900, Acts Loss: 0.0053, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 75/900, Acts Loss: 0.0061, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 76/900, Acts Loss: 0.0056, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 77/900, Acts Loss: 0.0052, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 78/900, Acts Loss: 0.0054, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 79/900, Acts Loss: 0.0037, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 80/900, Acts Loss: 0.0042, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 81/900, Acts Loss: 0.0057, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 82/900, Acts Loss: 0.0042, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 83/900, Acts Loss: 0.0055, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 84/900, Acts Loss: 0.0030, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 85/900, Acts Loss: 0.0051, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 86/900, Acts Loss: 0.0045, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 87/900, Acts Loss: 0.0053, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 88/900, Acts Loss: 0.0053, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 89/900, Acts Loss: 0.0042, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 90/900, Acts Loss: 0.0049, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 91/900, Acts Loss: 0.0042, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 92/900, Acts Loss: 0.0056, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 93/900, Acts Loss: 0.0048, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 94/900, Acts Loss: 0.0079, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 95/900, Acts Loss: 0.0053, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 96/900, Acts Loss: 0.0059, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 97/900, Acts Loss: 0.0064, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 98/900, Acts Loss: 0.0052, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 99/900, Acts Loss: 0.0058, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 100/900, Acts Loss: 0.0041, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 101/900, Acts Loss: 0.0052, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 102/900, Acts Loss: 0.0052, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 103/900, Acts Loss: 0.0046, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 104/900, Acts Loss: 0.0063, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 105/900, Acts Loss: 0.0051, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 106/900, Acts Loss: 0.0058, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 107/900, Acts Loss: 0.0073, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 108/900, Acts Loss: 0.0039, Logits Loss: 0.0016\n",
      "Epoch 1/1, Batch 109/900, Acts Loss: 0.0052, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 110/900, Acts Loss: 0.0036, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 111/900, Acts Loss: 0.0042, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 112/900, Acts Loss: 0.0051, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 113/900, Acts Loss: 0.0045, Logits Loss: 0.0017\n",
      "Epoch 1/1, Batch 114/900, Acts Loss: 0.0049, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 115/900, Acts Loss: 0.0062, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 116/900, Acts Loss: 0.0055, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 117/900, Acts Loss: 0.0062, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 118/900, Acts Loss: 0.0052, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 119/900, Acts Loss: 0.0053, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 120/900, Acts Loss: 0.0065, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 121/900, Acts Loss: 0.0049, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 122/900, Acts Loss: 0.0067, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 123/900, Acts Loss: 0.0063, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 124/900, Acts Loss: 0.0063, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 125/900, Acts Loss: 0.0047, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 126/900, Acts Loss: 0.0062, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 127/900, Acts Loss: 0.0045, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 128/900, Acts Loss: 0.0051, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 129/900, Acts Loss: 0.0034, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 130/900, Acts Loss: 0.0044, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 131/900, Acts Loss: 0.0028, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 132/900, Acts Loss: 0.0054, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 133/900, Acts Loss: 0.0056, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 134/900, Acts Loss: 0.0056, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 135/900, Acts Loss: 0.0048, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 136/900, Acts Loss: 0.0056, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 137/900, Acts Loss: 0.0058, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 138/900, Acts Loss: 0.0054, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 139/900, Acts Loss: 0.0070, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 140/900, Acts Loss: 0.0043, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 141/900, Acts Loss: 0.0056, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 142/900, Acts Loss: 0.0049, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 143/900, Acts Loss: 0.0047, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 144/900, Acts Loss: 0.0031, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 145/900, Acts Loss: 0.0027, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 146/900, Acts Loss: 0.0044, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 147/900, Acts Loss: 0.0057, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 148/900, Acts Loss: 0.0046, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 149/900, Acts Loss: 0.0039, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 150/900, Acts Loss: 0.0055, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 151/900, Acts Loss: 0.0039, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 152/900, Acts Loss: 0.0052, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 153/900, Acts Loss: 0.0029, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 154/900, Acts Loss: 0.0046, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 155/900, Acts Loss: 0.0045, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 156/900, Acts Loss: 0.0042, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 157/900, Acts Loss: 0.0039, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 158/900, Acts Loss: 0.0034, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 159/900, Acts Loss: 0.0033, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 160/900, Acts Loss: 0.0063, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 161/900, Acts Loss: 0.0046, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 162/900, Acts Loss: 0.0055, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 163/900, Acts Loss: 0.0036, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 164/900, Acts Loss: 0.0045, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 165/900, Acts Loss: 0.0048, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 166/900, Acts Loss: 0.0038, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 167/900, Acts Loss: 0.0025, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 168/900, Acts Loss: 0.0049, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 169/900, Acts Loss: 0.0037, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 170/900, Acts Loss: 0.0045, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 171/900, Acts Loss: 0.0052, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 172/900, Acts Loss: 0.0040, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 173/900, Acts Loss: 0.0029, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 174/900, Acts Loss: 0.0048, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 175/900, Acts Loss: 0.0038, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 176/900, Acts Loss: 0.0052, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 177/900, Acts Loss: 0.0065, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 178/900, Acts Loss: 0.0050, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 179/900, Acts Loss: 0.0029, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 180/900, Acts Loss: 0.0024, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 181/900, Acts Loss: 0.0051, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 182/900, Acts Loss: 0.0023, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 183/900, Acts Loss: 0.0046, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 184/900, Acts Loss: 0.0046, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 185/900, Acts Loss: 0.0050, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 186/900, Acts Loss: 0.0044, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 187/900, Acts Loss: 0.0043, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 188/900, Acts Loss: 0.0055, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 189/900, Acts Loss: 0.0055, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 190/900, Acts Loss: 0.0059, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 191/900, Acts Loss: 0.0032, Logits Loss: 0.0017\n",
      "Epoch 1/1, Batch 192/900, Acts Loss: 0.0026, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 193/900, Acts Loss: 0.0041, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 194/900, Acts Loss: 0.0054, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 195/900, Acts Loss: 0.0044, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 196/900, Acts Loss: 0.0052, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 197/900, Acts Loss: 0.0032, Logits Loss: 0.0016\n",
      "Epoch 1/1, Batch 198/900, Acts Loss: 0.0052, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 199/900, Acts Loss: 0.0055, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 200/900, Acts Loss: 0.0064, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 201/900, Acts Loss: 0.0038, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 202/900, Acts Loss: 0.0059, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 203/900, Acts Loss: 0.0054, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 204/900, Acts Loss: 0.0055, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 205/900, Acts Loss: 0.0034, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 206/900, Acts Loss: 0.0038, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 207/900, Acts Loss: 0.0062, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 208/900, Acts Loss: 0.0059, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 209/900, Acts Loss: 0.0040, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 210/900, Acts Loss: 0.0056, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 211/900, Acts Loss: 0.0057, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 212/900, Acts Loss: 0.0055, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 213/900, Acts Loss: 0.0052, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 214/900, Acts Loss: 0.0067, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 215/900, Acts Loss: 0.0060, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 216/900, Acts Loss: 0.0056, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 217/900, Acts Loss: 0.0051, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 218/900, Acts Loss: 0.0049, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 219/900, Acts Loss: 0.0063, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 220/900, Acts Loss: 0.0052, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 221/900, Acts Loss: 0.0076, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 222/900, Acts Loss: 0.0042, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 223/900, Acts Loss: 0.0048, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 224/900, Acts Loss: 0.0054, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 225/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 226/900, Acts Loss: 0.0052, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 227/900, Acts Loss: 0.0049, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 228/900, Acts Loss: 0.0065, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 229/900, Acts Loss: 0.0092, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 230/900, Acts Loss: 0.0062, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 231/900, Acts Loss: 0.0222, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 232/900, Acts Loss: 0.0090, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 233/900, Acts Loss: 0.0219, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 234/900, Acts Loss: 0.0079, Logits Loss: 0.0036\n",
      "Epoch 1/1, Batch 235/900, Acts Loss: 0.0077, Logits Loss: 0.0038\n",
      "Epoch 1/1, Batch 236/900, Acts Loss: 0.0094, Logits Loss: 0.0043\n",
      "Epoch 1/1, Batch 237/900, Acts Loss: 0.0073, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 238/900, Acts Loss: 0.0121, Logits Loss: 0.0075\n",
      "Epoch 1/1, Batch 239/900, Acts Loss: 0.0122, Logits Loss: 0.0077\n",
      "Epoch 1/1, Batch 240/900, Acts Loss: 0.0120, Logits Loss: 0.0064\n",
      "Epoch 1/1, Batch 241/900, Acts Loss: 0.0114, Logits Loss: 0.0058\n",
      "Epoch 1/1, Batch 242/900, Acts Loss: 0.0096, Logits Loss: 0.0036\n",
      "Epoch 1/1, Batch 243/900, Acts Loss: 0.0081, Logits Loss: 0.0038\n",
      "Epoch 1/1, Batch 244/900, Acts Loss: 0.0071, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 245/900, Acts Loss: 0.0088, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 246/900, Acts Loss: 0.0082, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 247/900, Acts Loss: 0.0092, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 248/900, Acts Loss: 0.0098, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 249/900, Acts Loss: 0.0078, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 250/900, Acts Loss: 0.0128, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 251/900, Acts Loss: 0.0084, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 252/900, Acts Loss: 0.0092, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 253/900, Acts Loss: 0.0408, Logits Loss: 0.0042\n",
      "Epoch 1/1, Batch 254/900, Acts Loss: 0.0095, Logits Loss: 0.0036\n",
      "Epoch 1/1, Batch 255/900, Acts Loss: 0.0095, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 256/900, Acts Loss: 0.0081, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 257/900, Acts Loss: 0.0092, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 258/900, Acts Loss: 0.0084, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 259/900, Acts Loss: 0.0070, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 260/900, Acts Loss: 0.0068, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 261/900, Acts Loss: 0.0085, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 262/900, Acts Loss: 0.0076, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 263/900, Acts Loss: 0.0070, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 264/900, Acts Loss: 0.0074, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 265/900, Acts Loss: 0.0073, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 266/900, Acts Loss: 0.0469, Logits Loss: 0.0071\n",
      "Epoch 1/1, Batch 267/900, Acts Loss: 0.0432, Logits Loss: 0.0059\n",
      "Epoch 1/1, Batch 268/900, Acts Loss: 0.0420, Logits Loss: 0.0043\n",
      "Epoch 1/1, Batch 269/900, Acts Loss: 0.0138, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 270/900, Acts Loss: 0.0386, Logits Loss: 0.0045\n",
      "Epoch 1/1, Batch 271/900, Acts Loss: 0.0374, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 272/900, Acts Loss: 0.0142, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 273/900, Acts Loss: 0.0344, Logits Loss: 0.0046\n",
      "Epoch 1/1, Batch 274/900, Acts Loss: 0.0371, Logits Loss: 0.0038\n",
      "Epoch 1/1, Batch 275/900, Acts Loss: 0.0112, Logits Loss: 0.0035\n",
      "Epoch 1/1, Batch 276/900, Acts Loss: 0.0115, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 277/900, Acts Loss: 0.0109, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 278/900, Acts Loss: 0.0354, Logits Loss: 0.0038\n",
      "Epoch 1/1, Batch 279/900, Acts Loss: 0.0106, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 280/900, Acts Loss: 0.0095, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 281/900, Acts Loss: 0.0117, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 282/900, Acts Loss: 0.0093, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 283/900, Acts Loss: 0.0106, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 284/900, Acts Loss: 0.0092, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 285/900, Acts Loss: 0.0087, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 286/900, Acts Loss: 0.0077, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 287/900, Acts Loss: 0.0084, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 288/900, Acts Loss: 0.0183, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 289/900, Acts Loss: 0.0106, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 290/900, Acts Loss: 0.0103, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 291/900, Acts Loss: 0.0186, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 292/900, Acts Loss: 0.0073, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 293/900, Acts Loss: 0.0243, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 294/900, Acts Loss: 0.0315, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 295/900, Acts Loss: 0.0091, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 296/900, Acts Loss: 0.0175, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 297/900, Acts Loss: 0.0164, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 298/900, Acts Loss: 0.0072, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 299/900, Acts Loss: 0.0056, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 300/900, Acts Loss: 0.0063, Logits Loss: 0.0037\n",
      "Epoch 1/1, Batch 301/900, Acts Loss: 0.0082, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 302/900, Acts Loss: 0.0109, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 303/900, Acts Loss: 0.0078, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 304/900, Acts Loss: 0.0079, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 305/900, Acts Loss: 0.0080, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 306/900, Acts Loss: 0.0083, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 307/900, Acts Loss: 0.0070, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 308/900, Acts Loss: 0.0085, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 309/900, Acts Loss: 0.0128, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 310/900, Acts Loss: 0.0072, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 311/900, Acts Loss: 0.0074, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 312/900, Acts Loss: 0.0078, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 313/900, Acts Loss: 0.0067, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 314/900, Acts Loss: 0.0052, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 315/900, Acts Loss: 0.0087, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 316/900, Acts Loss: 0.0099, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 317/900, Acts Loss: 0.0084, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 318/900, Acts Loss: 0.0068, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 319/900, Acts Loss: 0.0069, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 320/900, Acts Loss: 0.0067, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 321/900, Acts Loss: 0.0090, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 322/900, Acts Loss: 0.0204, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 323/900, Acts Loss: 0.0043, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 324/900, Acts Loss: 0.0093, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 325/900, Acts Loss: 0.0064, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 326/900, Acts Loss: 0.0082, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 327/900, Acts Loss: 0.0334, Logits Loss: 0.0042\n",
      "Epoch 1/1, Batch 328/900, Acts Loss: 0.0203, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 329/900, Acts Loss: 0.0060, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 330/900, Acts Loss: 0.0043, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 331/900, Acts Loss: 0.0045, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 332/900, Acts Loss: 0.0068, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 333/900, Acts Loss: 0.0059, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 334/900, Acts Loss: 0.0053, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 335/900, Acts Loss: 0.0044, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 336/900, Acts Loss: 0.0085, Logits Loss: 0.0042\n",
      "Epoch 1/1, Batch 337/900, Acts Loss: 0.0081, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 338/900, Acts Loss: 0.0054, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 339/900, Acts Loss: 0.0074, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 340/900, Acts Loss: 0.0078, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 341/900, Acts Loss: 0.0046, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 342/900, Acts Loss: 0.0052, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 343/900, Acts Loss: 0.0095, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 344/900, Acts Loss: 0.0120, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 345/900, Acts Loss: 0.0077, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 346/900, Acts Loss: 0.0091, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 347/900, Acts Loss: 0.0069, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 348/900, Acts Loss: 0.0366, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 349/900, Acts Loss: 0.0378, Logits Loss: 0.0043\n",
      "Epoch 1/1, Batch 350/900, Acts Loss: 0.0298, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 351/900, Acts Loss: 0.0079, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 352/900, Acts Loss: 0.0276, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 353/900, Acts Loss: 0.0072, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 354/900, Acts Loss: 0.0282, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 355/900, Acts Loss: 0.0067, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 356/900, Acts Loss: 0.0079, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 357/900, Acts Loss: 0.0101, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 358/900, Acts Loss: 0.0048, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 359/900, Acts Loss: 0.0049, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 360/900, Acts Loss: 0.0071, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 361/900, Acts Loss: 0.0050, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 362/900, Acts Loss: 0.0067, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 363/900, Acts Loss: 0.0052, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 364/900, Acts Loss: 0.0103, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 365/900, Acts Loss: 0.0051, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 366/900, Acts Loss: 0.0104, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 367/900, Acts Loss: 0.0053, Logits Loss: 0.0036\n",
      "Epoch 1/1, Batch 368/900, Acts Loss: 0.0063, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 369/900, Acts Loss: 0.0078, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 370/900, Acts Loss: 0.0049, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 371/900, Acts Loss: 0.0376, Logits Loss: 0.0045\n",
      "Epoch 1/1, Batch 372/900, Acts Loss: 0.0330, Logits Loss: 0.0043\n",
      "Epoch 1/1, Batch 373/900, Acts Loss: 0.0298, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 374/900, Acts Loss: 0.0125, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 375/900, Acts Loss: 0.0332, Logits Loss: 0.0039\n",
      "Epoch 1/1, Batch 376/900, Acts Loss: 0.0057, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 377/900, Acts Loss: 0.0298, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 378/900, Acts Loss: 0.0298, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 379/900, Acts Loss: 0.0293, Logits Loss: 0.0036\n",
      "Epoch 1/1, Batch 380/900, Acts Loss: 0.0082, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 381/900, Acts Loss: 0.0051, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 382/900, Acts Loss: 0.0071, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 383/900, Acts Loss: 0.0081, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 384/900, Acts Loss: 0.0098, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 385/900, Acts Loss: 0.0058, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 386/900, Acts Loss: 0.0088, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 387/900, Acts Loss: 0.0088, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 388/900, Acts Loss: 0.0111, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 389/900, Acts Loss: 0.0049, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 390/900, Acts Loss: 0.0040, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 391/900, Acts Loss: 0.0045, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 392/900, Acts Loss: 0.0060, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 393/900, Acts Loss: 0.0048, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 394/900, Acts Loss: 0.0067, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 395/900, Acts Loss: 0.0052, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 396/900, Acts Loss: 0.0039, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 397/900, Acts Loss: 0.0052, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 398/900, Acts Loss: 0.0053, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 399/900, Acts Loss: 0.0044, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 400/900, Acts Loss: 0.0044, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 401/900, Acts Loss: 0.0067, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 402/900, Acts Loss: 0.0038, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 403/900, Acts Loss: 0.0046, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 404/900, Acts Loss: 0.0062, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 405/900, Acts Loss: 0.0039, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 406/900, Acts Loss: 0.0040, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 407/900, Acts Loss: 0.0069, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 408/900, Acts Loss: 0.0042, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 409/900, Acts Loss: 0.0041, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 410/900, Acts Loss: 0.0040, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 411/900, Acts Loss: 0.0041, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 412/900, Acts Loss: 0.0046, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 413/900, Acts Loss: 0.0060, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 414/900, Acts Loss: 0.0066, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 415/900, Acts Loss: 0.0065, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 416/900, Acts Loss: 0.0043, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 417/900, Acts Loss: 0.0042, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 418/900, Acts Loss: 0.0035, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 419/900, Acts Loss: 0.0041, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 420/900, Acts Loss: 0.0074, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 421/900, Acts Loss: 0.0042, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 422/900, Acts Loss: 0.0087, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 423/900, Acts Loss: 0.0051, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 424/900, Acts Loss: 0.0064, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 425/900, Acts Loss: 0.0048, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 426/900, Acts Loss: 0.0041, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 427/900, Acts Loss: 0.0046, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 428/900, Acts Loss: 0.0046, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 429/900, Acts Loss: 0.0065, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 430/900, Acts Loss: 0.0078, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 431/900, Acts Loss: 0.0039, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 432/900, Acts Loss: 0.0074, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 433/900, Acts Loss: 0.0062, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 434/900, Acts Loss: 0.0038, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 435/900, Acts Loss: 0.0043, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 436/900, Acts Loss: 0.0048, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 437/900, Acts Loss: 0.0034, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 438/900, Acts Loss: 0.0071, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 439/900, Acts Loss: 0.0041, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 440/900, Acts Loss: 0.0048, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 441/900, Acts Loss: 0.0034, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 442/900, Acts Loss: 0.0038, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 443/900, Acts Loss: 0.0040, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 444/900, Acts Loss: 0.0033, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 445/900, Acts Loss: 0.0031, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 446/900, Acts Loss: 0.0099, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 447/900, Acts Loss: 0.0043, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 448/900, Acts Loss: 0.0036, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 449/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 450/900, Acts Loss: 0.0094, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 451/900, Acts Loss: 0.0063, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 452/900, Acts Loss: 0.0030, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 453/900, Acts Loss: 0.0075, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 454/900, Acts Loss: 0.0060, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 455/900, Acts Loss: 0.0045, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 456/900, Acts Loss: 0.0065, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 457/900, Acts Loss: 0.0039, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 458/900, Acts Loss: 0.0034, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 459/900, Acts Loss: 0.0074, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 460/900, Acts Loss: 0.0062, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 461/900, Acts Loss: 0.0030, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 462/900, Acts Loss: 0.0036, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 463/900, Acts Loss: 0.0033, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 464/900, Acts Loss: 0.0038, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 465/900, Acts Loss: 0.0034, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 466/900, Acts Loss: 0.0041, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 467/900, Acts Loss: 0.0064, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 468/900, Acts Loss: 0.0066, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 469/900, Acts Loss: 0.0067, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 470/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 471/900, Acts Loss: 0.0068, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 472/900, Acts Loss: 0.0054, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 473/900, Acts Loss: 0.0037, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 474/900, Acts Loss: 0.0066, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 475/900, Acts Loss: 0.0037, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 476/900, Acts Loss: 0.0056, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 477/900, Acts Loss: 0.0030, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 478/900, Acts Loss: 0.0029, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 479/900, Acts Loss: 0.0036, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 480/900, Acts Loss: 0.0046, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 481/900, Acts Loss: 0.0056, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 482/900, Acts Loss: 0.0071, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 483/900, Acts Loss: 0.0037, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 484/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 485/900, Acts Loss: 0.0028, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 486/900, Acts Loss: 0.0038, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 487/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 488/900, Acts Loss: 0.0054, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 489/900, Acts Loss: 0.0059, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 490/900, Acts Loss: 0.0029, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 491/900, Acts Loss: 0.0036, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 492/900, Acts Loss: 0.0031, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 493/900, Acts Loss: 0.0061, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 494/900, Acts Loss: 0.0032, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 495/900, Acts Loss: 0.0035, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 496/900, Acts Loss: 0.0028, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 497/900, Acts Loss: 0.0031, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 498/900, Acts Loss: 0.0060, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 499/900, Acts Loss: 0.0030, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 500/900, Acts Loss: 0.0040, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 501/900, Acts Loss: 0.0069, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 502/900, Acts Loss: 0.0056, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 503/900, Acts Loss: 0.0051, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 504/900, Acts Loss: 0.0032, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 505/900, Acts Loss: 0.0056, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 506/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 507/900, Acts Loss: 0.0028, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 508/900, Acts Loss: 0.0053, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 509/900, Acts Loss: 0.0060, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 510/900, Acts Loss: 0.0054, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 511/900, Acts Loss: 0.0029, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 512/900, Acts Loss: 0.0067, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 513/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 514/900, Acts Loss: 0.0034, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 515/900, Acts Loss: 0.0040, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 516/900, Acts Loss: 0.0070, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 517/900, Acts Loss: 0.0035, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 518/900, Acts Loss: 0.0036, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 519/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 520/900, Acts Loss: 0.0074, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 521/900, Acts Loss: 0.0057, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 522/900, Acts Loss: 0.0028, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 523/900, Acts Loss: 0.0040, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 524/900, Acts Loss: 0.0029, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 525/900, Acts Loss: 0.0069, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 526/900, Acts Loss: 0.0036, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 527/900, Acts Loss: 0.0069, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 528/900, Acts Loss: 0.0036, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 529/900, Acts Loss: 0.0057, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 530/900, Acts Loss: 0.0038, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 531/900, Acts Loss: 0.0032, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 532/900, Acts Loss: 0.0029, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 533/900, Acts Loss: 0.0033, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 534/900, Acts Loss: 0.0039, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 535/900, Acts Loss: 0.0037, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 536/900, Acts Loss: 0.0037, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 537/900, Acts Loss: 0.0075, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 538/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 539/900, Acts Loss: 0.0041, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 540/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 541/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 542/900, Acts Loss: 0.0037, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 543/900, Acts Loss: 0.0065, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 544/900, Acts Loss: 0.0036, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 545/900, Acts Loss: 0.0036, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 546/900, Acts Loss: 0.0027, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 547/900, Acts Loss: 0.0034, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 548/900, Acts Loss: 0.0035, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 549/900, Acts Loss: 0.0040, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 550/900, Acts Loss: 0.0065, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 551/900, Acts Loss: 0.0031, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 552/900, Acts Loss: 0.0038, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 553/900, Acts Loss: 0.0036, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 554/900, Acts Loss: 0.0034, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 555/900, Acts Loss: 0.0033, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 556/900, Acts Loss: 0.0029, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 557/900, Acts Loss: 0.0026, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 558/900, Acts Loss: 0.0033, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 559/900, Acts Loss: 0.0028, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 560/900, Acts Loss: 0.0058, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 561/900, Acts Loss: 0.0028, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 562/900, Acts Loss: 0.0029, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 563/900, Acts Loss: 0.0033, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 564/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 565/900, Acts Loss: 0.0028, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 566/900, Acts Loss: 0.0038, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 567/900, Acts Loss: 0.0062, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 568/900, Acts Loss: 0.0058, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 569/900, Acts Loss: 0.0037, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 570/900, Acts Loss: 0.0039, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 571/900, Acts Loss: 0.0032, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 572/900, Acts Loss: 0.0026, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 573/900, Acts Loss: 0.0032, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 574/900, Acts Loss: 0.0060, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 575/900, Acts Loss: 0.0055, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 576/900, Acts Loss: 0.0062, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 577/900, Acts Loss: 0.0029, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 578/900, Acts Loss: 0.0039, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 579/900, Acts Loss: 0.0064, Logits Loss: 0.0017\n",
      "Epoch 1/1, Batch 580/900, Acts Loss: 0.0037, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 581/900, Acts Loss: 0.0070, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 582/900, Acts Loss: 0.0048, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 583/900, Acts Loss: 0.0035, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 584/900, Acts Loss: 0.0066, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 585/900, Acts Loss: 0.0085, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 586/900, Acts Loss: 0.0061, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 587/900, Acts Loss: 0.0027, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 588/900, Acts Loss: 0.0060, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 589/900, Acts Loss: 0.0031, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 590/900, Acts Loss: 0.0070, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 591/900, Acts Loss: 0.0039, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 592/900, Acts Loss: 0.0027, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 593/900, Acts Loss: 0.0055, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 594/900, Acts Loss: 0.0041, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 595/900, Acts Loss: 0.0034, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 596/900, Acts Loss: 0.0031, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 597/900, Acts Loss: 0.0058, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 598/900, Acts Loss: 0.0031, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 599/900, Acts Loss: 0.0039, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 600/900, Acts Loss: 0.0030, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 601/900, Acts Loss: 0.0033, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 602/900, Acts Loss: 0.0052, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 603/900, Acts Loss: 0.0036, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 604/900, Acts Loss: 0.0051, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 605/900, Acts Loss: 0.0036, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 606/900, Acts Loss: 0.0028, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 607/900, Acts Loss: 0.0065, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 608/900, Acts Loss: 0.0055, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 609/900, Acts Loss: 0.0045, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 610/900, Acts Loss: 0.0054, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 611/900, Acts Loss: 0.0048, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 612/900, Acts Loss: 0.0058, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 613/900, Acts Loss: 0.0068, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 614/900, Acts Loss: 0.0031, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 615/900, Acts Loss: 0.0062, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 616/900, Acts Loss: 0.0030, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 617/900, Acts Loss: 0.0057, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 618/900, Acts Loss: 0.0037, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 619/900, Acts Loss: 0.0060, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 620/900, Acts Loss: 0.0055, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 621/900, Acts Loss: 0.0039, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 622/900, Acts Loss: 0.0029, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 623/900, Acts Loss: 0.0177, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 624/900, Acts Loss: 0.0031, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 625/900, Acts Loss: 0.0030, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 626/900, Acts Loss: 0.0037, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 627/900, Acts Loss: 0.0056, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 628/900, Acts Loss: 0.0028, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 629/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 630/900, Acts Loss: 0.0038, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 631/900, Acts Loss: 0.0038, Logits Loss: 0.0016\n",
      "Epoch 1/1, Batch 632/900, Acts Loss: 0.0036, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 633/900, Acts Loss: 0.0052, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 634/900, Acts Loss: 0.0034, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 635/900, Acts Loss: 0.0055, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 636/900, Acts Loss: 0.0064, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 637/900, Acts Loss: 0.0033, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 638/900, Acts Loss: 0.0056, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 639/900, Acts Loss: 0.0060, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 640/900, Acts Loss: 0.0036, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 641/900, Acts Loss: 0.0033, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 642/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 643/900, Acts Loss: 0.0036, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 644/900, Acts Loss: 0.0054, Logits Loss: 0.0017\n",
      "Epoch 1/1, Batch 645/900, Acts Loss: 0.0041, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 646/900, Acts Loss: 0.0045, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 647/900, Acts Loss: 0.0049, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 648/900, Acts Loss: 0.0034, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 649/900, Acts Loss: 0.0062, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 650/900, Acts Loss: 0.0050, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 651/900, Acts Loss: 0.0063, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 652/900, Acts Loss: 0.0037, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 653/900, Acts Loss: 0.0034, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 654/900, Acts Loss: 0.0030, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 655/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 656/900, Acts Loss: 0.0033, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 657/900, Acts Loss: 0.0029, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 658/900, Acts Loss: 0.0067, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 659/900, Acts Loss: 0.0038, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 660/900, Acts Loss: 0.0028, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 661/900, Acts Loss: 0.0093, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 662/900, Acts Loss: 0.0063, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 663/900, Acts Loss: 0.0037, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 664/900, Acts Loss: 0.0038, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 665/900, Acts Loss: 0.0079, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 666/900, Acts Loss: 0.0092, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 667/900, Acts Loss: 0.0031, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 668/900, Acts Loss: 0.0038, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 669/900, Acts Loss: 0.0029, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 670/900, Acts Loss: 0.0036, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 671/900, Acts Loss: 0.0032, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 672/900, Acts Loss: 0.0078, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 673/900, Acts Loss: 0.0111, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 674/900, Acts Loss: 0.0087, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 675/900, Acts Loss: 0.0041, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 676/900, Acts Loss: 0.0038, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 677/900, Acts Loss: 0.0036, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 678/900, Acts Loss: 0.0034, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 679/900, Acts Loss: 0.0043, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 680/900, Acts Loss: 0.0041, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 681/900, Acts Loss: 0.0042, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 682/900, Acts Loss: 0.0038, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 683/900, Acts Loss: 0.0039, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 684/900, Acts Loss: 0.0038, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 685/900, Acts Loss: 0.0039, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 686/900, Acts Loss: 0.0055, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 687/900, Acts Loss: 0.0040, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 688/900, Acts Loss: 0.0041, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 689/900, Acts Loss: 0.0038, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 690/900, Acts Loss: 0.0055, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 691/900, Acts Loss: 0.0040, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 692/900, Acts Loss: 0.0042, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 693/900, Acts Loss: 0.0045, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 694/900, Acts Loss: 0.0046, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 695/900, Acts Loss: 0.0049, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 696/900, Acts Loss: 0.0046, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 697/900, Acts Loss: 0.0050, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 698/900, Acts Loss: 0.0048, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 699/900, Acts Loss: 0.0049, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 700/900, Acts Loss: 0.0056, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 701/900, Acts Loss: 0.0079, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 702/900, Acts Loss: 0.0056, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 703/900, Acts Loss: 0.0109, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 704/900, Acts Loss: 0.0073, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 705/900, Acts Loss: 0.0084, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 706/900, Acts Loss: 0.0075, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 707/900, Acts Loss: 0.0098, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 708/900, Acts Loss: 0.0046, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 709/900, Acts Loss: 0.0065, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 710/900, Acts Loss: 0.0077, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 711/900, Acts Loss: 0.0065, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 712/900, Acts Loss: 0.0040, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 713/900, Acts Loss: 0.0078, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 714/900, Acts Loss: 0.0049, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 715/900, Acts Loss: 0.0047, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 716/900, Acts Loss: 0.0056, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 717/900, Acts Loss: 0.0067, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 718/900, Acts Loss: 0.0067, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 719/900, Acts Loss: 0.0076, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 720/900, Acts Loss: 0.0034, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 721/900, Acts Loss: 0.0042, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 722/900, Acts Loss: 0.0052, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 723/900, Acts Loss: 0.0031, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 724/900, Acts Loss: 0.0075, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 725/900, Acts Loss: 0.0041, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 726/900, Acts Loss: 0.0032, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 727/900, Acts Loss: 0.0041, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 728/900, Acts Loss: 0.0042, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 729/900, Acts Loss: 0.0036, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 730/900, Acts Loss: 0.0042, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 731/900, Acts Loss: 0.0037, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 732/900, Acts Loss: 0.0039, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 733/900, Acts Loss: 0.0110, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 734/900, Acts Loss: 0.0046, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 735/900, Acts Loss: 0.0069, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 736/900, Acts Loss: 0.0039, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 737/900, Acts Loss: 0.0061, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 738/900, Acts Loss: 0.0064, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 739/900, Acts Loss: 0.0045, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 740/900, Acts Loss: 0.0056, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 741/900, Acts Loss: 0.0041, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 742/900, Acts Loss: 0.0060, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 743/900, Acts Loss: 0.0074, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 744/900, Acts Loss: 0.0064, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 745/900, Acts Loss: 0.0074, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 746/900, Acts Loss: 0.0053, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 747/900, Acts Loss: 0.0034, Logits Loss: 0.0016\n",
      "Epoch 1/1, Batch 748/900, Acts Loss: 0.0049, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 749/900, Acts Loss: 0.0041, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 750/900, Acts Loss: 0.0083, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 751/900, Acts Loss: 0.0077, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 752/900, Acts Loss: 0.0037, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 753/900, Acts Loss: 0.0061, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 754/900, Acts Loss: 0.0038, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 755/900, Acts Loss: 0.0060, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 756/900, Acts Loss: 0.0081, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 757/900, Acts Loss: 0.0036, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 758/900, Acts Loss: 0.0036, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 759/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 760/900, Acts Loss: 0.0034, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 761/900, Acts Loss: 0.0033, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 762/900, Acts Loss: 0.0035, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 763/900, Acts Loss: 0.0043, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 764/900, Acts Loss: 0.0063, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 765/900, Acts Loss: 0.0032, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 766/900, Acts Loss: 0.0027, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 767/900, Acts Loss: 0.0033, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 768/900, Acts Loss: 0.0057, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 769/900, Acts Loss: 0.0050, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 770/900, Acts Loss: 0.0031, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 771/900, Acts Loss: 0.0072, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 772/900, Acts Loss: 0.0055, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 773/900, Acts Loss: 0.0038, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 774/900, Acts Loss: 0.0035, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 775/900, Acts Loss: 0.0045, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 776/900, Acts Loss: 0.0041, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 777/900, Acts Loss: 0.0064, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 778/900, Acts Loss: 0.0031, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 779/900, Acts Loss: 0.0069, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 780/900, Acts Loss: 0.0031, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 781/900, Acts Loss: 0.0073, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 782/900, Acts Loss: 0.0029, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 783/900, Acts Loss: 0.0031, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 784/900, Acts Loss: 0.0032, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 785/900, Acts Loss: 0.0030, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 786/900, Acts Loss: 0.0034, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 787/900, Acts Loss: 0.0048, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 788/900, Acts Loss: 0.0065, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 789/900, Acts Loss: 0.0033, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 790/900, Acts Loss: 0.0070, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 791/900, Acts Loss: 0.0047, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 792/900, Acts Loss: 0.0050, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 793/900, Acts Loss: 0.0045, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 794/900, Acts Loss: 0.0033, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 795/900, Acts Loss: 0.0043, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 796/900, Acts Loss: 0.0032, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 797/900, Acts Loss: 0.0042, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 798/900, Acts Loss: 0.0029, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 799/900, Acts Loss: 0.0029, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 800/900, Acts Loss: 0.0039, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 801/900, Acts Loss: 0.0043, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 802/900, Acts Loss: 0.0035, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 803/900, Acts Loss: 0.0034, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 804/900, Acts Loss: 0.0037, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 805/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 806/900, Acts Loss: 0.0026, Logits Loss: 0.0018\n",
      "Epoch 1/1, Batch 807/900, Acts Loss: 0.0042, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 808/900, Acts Loss: 0.0045, Logits Loss: 0.0020\n",
      "Epoch 1/1, Batch 809/900, Acts Loss: 0.0042, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 810/900, Acts Loss: 0.0028, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 811/900, Acts Loss: 0.0067, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 812/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 813/900, Acts Loss: 0.0055, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 814/900, Acts Loss: 0.0052, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 815/900, Acts Loss: 0.0029, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 816/900, Acts Loss: 0.0073, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 817/900, Acts Loss: 0.0060, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 818/900, Acts Loss: 0.0041, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 819/900, Acts Loss: 0.0025, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 820/900, Acts Loss: 0.0175, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 821/900, Acts Loss: 0.0038, Logits Loss: 0.0019\n",
      "Epoch 1/1, Batch 822/900, Acts Loss: 0.0070, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 823/900, Acts Loss: 0.0034, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 824/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 825/900, Acts Loss: 0.0042, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 826/900, Acts Loss: 0.0028, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 827/900, Acts Loss: 0.0034, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 828/900, Acts Loss: 0.0069, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 829/900, Acts Loss: 0.0066, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 830/900, Acts Loss: 0.0034, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 831/900, Acts Loss: 0.0034, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 832/900, Acts Loss: 0.0071, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 833/900, Acts Loss: 0.0038, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 834/900, Acts Loss: 0.0039, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 835/900, Acts Loss: 0.0070, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 836/900, Acts Loss: 0.0069, Logits Loss: 0.0021\n",
      "Epoch 1/1, Batch 837/900, Acts Loss: 0.0046, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 838/900, Acts Loss: 0.0037, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 839/900, Acts Loss: 0.0035, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 840/900, Acts Loss: 0.0033, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 841/900, Acts Loss: 0.0033, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 842/900, Acts Loss: 0.0039, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 843/900, Acts Loss: 0.0060, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 844/900, Acts Loss: 0.0057, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 845/900, Acts Loss: 0.0030, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 846/900, Acts Loss: 0.0043, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 847/900, Acts Loss: 0.0033, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 848/900, Acts Loss: 0.0045, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 849/900, Acts Loss: 0.0038, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 850/900, Acts Loss: 0.0026, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 851/900, Acts Loss: 0.0036, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 852/900, Acts Loss: 0.0060, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 853/900, Acts Loss: 0.0063, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 854/900, Acts Loss: 0.0041, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 855/900, Acts Loss: 0.0040, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 856/900, Acts Loss: 0.0038, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 857/900, Acts Loss: 0.0030, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 858/900, Acts Loss: 0.0131, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 859/900, Acts Loss: 0.0051, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 860/900, Acts Loss: 0.0042, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 861/900, Acts Loss: 0.0039, Logits Loss: 0.0025\n",
      "Epoch 1/1, Batch 862/900, Acts Loss: 0.0037, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 863/900, Acts Loss: 0.0037, Logits Loss: 0.0022\n",
      "Epoch 1/1, Batch 864/900, Acts Loss: 0.0058, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 865/900, Acts Loss: 0.0156, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 866/900, Acts Loss: 0.0049, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 867/900, Acts Loss: 0.0058, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 868/900, Acts Loss: 0.0060, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 869/900, Acts Loss: 0.0042, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 870/900, Acts Loss: 0.0049, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 871/900, Acts Loss: 0.0118, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 872/900, Acts Loss: 0.0047, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 873/900, Acts Loss: 0.0057, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 874/900, Acts Loss: 0.0053, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 875/900, Acts Loss: 0.0066, Logits Loss: 0.0023\n",
      "Epoch 1/1, Batch 876/900, Acts Loss: 0.0127, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 877/900, Acts Loss: 0.0095, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 878/900, Acts Loss: 0.0089, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 879/900, Acts Loss: 0.0088, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 880/900, Acts Loss: 0.0104, Logits Loss: 0.0037\n",
      "Epoch 1/1, Batch 881/900, Acts Loss: 0.0074, Logits Loss: 0.0030\n",
      "Epoch 1/1, Batch 882/900, Acts Loss: 0.0060, Logits Loss: 0.0035\n",
      "Epoch 1/1, Batch 883/900, Acts Loss: 0.0054, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 884/900, Acts Loss: 0.0054, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 885/900, Acts Loss: 0.0055, Logits Loss: 0.0031\n",
      "Epoch 1/1, Batch 886/900, Acts Loss: 0.0056, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 887/900, Acts Loss: 0.0060, Logits Loss: 0.0029\n",
      "Epoch 1/1, Batch 888/900, Acts Loss: 0.0068, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 889/900, Acts Loss: 0.0081, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 890/900, Acts Loss: 0.0063, Logits Loss: 0.0027\n",
      "Epoch 1/1, Batch 891/900, Acts Loss: 0.0064, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 892/900, Acts Loss: 0.0063, Logits Loss: 0.0032\n",
      "Epoch 1/1, Batch 893/900, Acts Loss: 0.0083, Logits Loss: 0.0040\n",
      "Epoch 1/1, Batch 894/900, Acts Loss: 0.0085, Logits Loss: 0.0026\n",
      "Epoch 1/1, Batch 895/900, Acts Loss: 0.0078, Logits Loss: 0.0034\n",
      "Epoch 1/1, Batch 896/900, Acts Loss: 0.0065, Logits Loss: 0.0033\n",
      "Epoch 1/1, Batch 897/900, Acts Loss: 0.0073, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 898/900, Acts Loss: 0.0061, Logits Loss: 0.0028\n",
      "Epoch 1/1, Batch 899/900, Acts Loss: 0.0067, Logits Loss: 0.0024\n",
      "Epoch 1/1, Batch 900/900, Acts Loss: 0.0096, Logits Loss: 0.0028\n",
      "Epoch 1/1, Loss: 0.0124\n",
      "Validation Epoch 1/1, Average Acts Loss: 0.0081, Average Logits Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "train_peft_model(m, num_epochs=1, learning_rate=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
