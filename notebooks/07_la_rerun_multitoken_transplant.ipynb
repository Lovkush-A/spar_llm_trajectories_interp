{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from taker import Model\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'microsoft/phi-3-mini-4k-instruct' with int4:\n",
      "- Added 512 hooks across 32 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "m = Model(model_name, dtype=\"int4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - n_layers : 32\n",
      " - d_model  : 3072\n",
      " - n_heads  : 32\n",
      " - d_head   : 96\n",
      " - d_mlp    : 8192\n"
     ]
    }
   ],
   "source": [
    "m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[869, 13, 13]\n",
      "['▁.', '<0x0A>', '<0x0A>']\n",
      "[16340, 270, 16697, 29889, 13, 13]\n",
      "['▁Sunday', '▁d', 'inners', '.', '<0x0A>', '<0x0A>']\n"
     ]
    }
   ],
   "source": [
    "idlist = m.get_ids(\".\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"Sunday dinners.\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short blog post about a recipe and the inspiration behind it.\n",
      " Do not include a title.\n",
      " Only reveal the dish after the story.\n",
      " Start with short story and then move to the recipe.\n",
      " To re-iterate, do not include a title.\n",
      "\n",
      " Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
      "\n",
      "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
      "\n",
      "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a short blog post about a recipe and the inspiration behind it.\n",
    " Do not include a title.\n",
    " Only reveal the dish after the story.\n",
    " Start with short story and then move to the recipe.\n",
    " To re-iterate, do not include a title.\"\"\"\n",
    "\n",
    "\n",
    "story = \"\"\"\n",
    "\\n Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
    "\n",
    "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
    "\n",
    "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\\n\\n\"\"\"\n",
    "\n",
    "prompt_original = prompt + story\n",
    "print(prompt_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_new='................................'\n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_transfer = 2\n",
    "\n",
    "idlist_original = m.get_ids(prompt_original).squeeze().tolist()\n",
    "tokens_original = m.tokenizer.convert_ids_to_tokens(idlist_original)\n",
    "n_tokens_original = len(tokens_original)\n",
    "\n",
    "# prompt is just string of dots.\n",
    "prompt_new = \"................\" * n_tokens_to_transfer\n",
    "\n",
    "# prefix = \"\"\n",
    "# tokens_to_transfer = tokens_original[-n_tokens_to_transfer:]\n",
    "# string_to_transfer = m.tokenizer.convert_tokens_to_string(tokens_to_transfer)\n",
    "# prompt_new = prefix + string_to_transfer\n",
    "print(f\"{prompt_new=}\")\n",
    "\n",
    "idlist_new = m.get_ids(prompt_new).squeeze().tolist()\n",
    "tokens_new = m.tokenizer.convert_ids_to_tokens(idlist_new)\n",
    "n_tokens_new = len(tokens_new)\n",
    "\n",
    "token_index_map = {\n",
    "    n_tokens_original\n",
    "    - n_tokens_to_transfer\n",
    "    + i: n_tokens_new\n",
    "    - n_tokens_to_transfer\n",
    "    + i\n",
    "    for i in range(n_tokens_to_transfer)\n",
    "}\n",
    "\n",
    "# # do sense check\n",
    "# for index_original, index_new in token_index_map.items():\n",
    "#     assert tokens_original[index_original] == tokens_new[index_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0_attn_pre_out': NeuronReplace(), 'layer_0_mlp_pre_out': NeuronReplace(), 'layer_1_attn_pre_out': NeuronReplace(), 'layer_1_mlp_pre_out': NeuronReplace(), 'layer_2_attn_pre_out': NeuronReplace(), 'layer_2_mlp_pre_out': NeuronReplace(), 'layer_3_attn_pre_out': NeuronReplace(), 'layer_3_mlp_pre_out': NeuronReplace(), 'layer_4_attn_pre_out': NeuronReplace(), 'layer_4_mlp_pre_out': NeuronReplace(), 'layer_5_attn_pre_out': NeuronReplace(), 'layer_5_mlp_pre_out': NeuronReplace(), 'layer_6_attn_pre_out': NeuronReplace(), 'layer_6_mlp_pre_out': NeuronReplace(), 'layer_7_attn_pre_out': NeuronReplace(), 'layer_7_mlp_pre_out': NeuronReplace(), 'layer_8_attn_pre_out': NeuronReplace(), 'layer_8_mlp_pre_out': NeuronReplace(), 'layer_9_attn_pre_out': NeuronReplace(), 'layer_9_mlp_pre_out': NeuronReplace(), 'layer_10_attn_pre_out': NeuronReplace(), 'layer_10_mlp_pre_out': NeuronReplace(), 'layer_11_attn_pre_out': NeuronReplace(), 'layer_11_mlp_pre_out': NeuronReplace(), 'layer_12_attn_pre_out': NeuronReplace(), 'layer_12_mlp_pre_out': NeuronReplace(), 'layer_13_attn_pre_out': NeuronReplace(), 'layer_13_mlp_pre_out': NeuronReplace(), 'layer_14_attn_pre_out': NeuronReplace(), 'layer_14_mlp_pre_out': NeuronReplace(), 'layer_15_attn_pre_out': NeuronReplace(), 'layer_15_mlp_pre_out': NeuronReplace(), 'layer_16_attn_pre_out': NeuronReplace(), 'layer_16_mlp_pre_out': NeuronReplace(), 'layer_17_attn_pre_out': NeuronReplace(), 'layer_17_mlp_pre_out': NeuronReplace(), 'layer_18_attn_pre_out': NeuronReplace(), 'layer_18_mlp_pre_out': NeuronReplace(), 'layer_19_attn_pre_out': NeuronReplace(), 'layer_19_mlp_pre_out': NeuronReplace(), 'layer_20_attn_pre_out': NeuronReplace(), 'layer_20_mlp_pre_out': NeuronReplace(), 'layer_21_attn_pre_out': NeuronReplace(), 'layer_21_mlp_pre_out': NeuronReplace(), 'layer_22_attn_pre_out': NeuronReplace(), 'layer_22_mlp_pre_out': NeuronReplace(), 'layer_23_attn_pre_out': NeuronReplace(), 'layer_23_mlp_pre_out': NeuronReplace(), 'layer_24_attn_pre_out': NeuronReplace(), 'layer_24_mlp_pre_out': NeuronReplace(), 'layer_25_attn_pre_out': NeuronReplace(), 'layer_25_mlp_pre_out': NeuronReplace(), 'layer_26_attn_pre_out': NeuronReplace(), 'layer_26_mlp_pre_out': NeuronReplace(), 'layer_27_attn_pre_out': NeuronReplace(), 'layer_27_mlp_pre_out': NeuronReplace(), 'layer_28_attn_pre_out': NeuronReplace(), 'layer_28_mlp_pre_out': NeuronReplace(), 'layer_29_attn_pre_out': NeuronReplace(), 'layer_29_mlp_pre_out': NeuronReplace(), 'layer_30_attn_pre_out': NeuronReplace(), 'layer_30_mlp_pre_out': NeuronReplace(), 'layer_31_attn_pre_out': NeuronReplace(), 'layer_31_mlp_pre_out': NeuronReplace()}\n"
     ]
    }
   ],
   "source": [
    "# RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "for h in m.hooks.neuron_replace.values():\n",
    "    h.reset()\n",
    "\n",
    "print(m.hooks.neuron_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_original = m.get_midlayer_activations(prompt_original)\n",
    "\n",
    "for original_index, new_index in token_index_map.items():\n",
    "    for layer_type in [\"mlp\", \"attn\"]:\n",
    "        for layer_number in range(16):\n",
    "            hook = m.hooks.neuron_replace[f\"layer_{layer_number}_{layer_type}_pre_out\"]\n",
    "            hook.add_token(\n",
    "                new_index,\n",
    "                activations_original[layer_type][0, layer_number, original_index],\n",
    "            )\n",
    "\n",
    "# for original_index, new_index in token_index_map.items():\n",
    "#     for name, hook in m.hooks.neuron_replace.items():\n",
    "#         # name is of the form \"layer_{layer_number}_{layer_type}_pre_out\"\n",
    "#         _, layer_number, layer_type, _, _ = name.split(\"_\")\n",
    "#         layer_number = int(layer_number)\n",
    "#         hook.add_token(new_index, activations_original[layer_type][0, layer_number, original_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"../results/{current_time}_agnes_multi_token_transfer_LA_tests.jsonl\"\n",
    "\n",
    "if not exists(filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HiddenPrints():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_new_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer-first-half-layers-alternative-loop--prompt-new-is-just-dots\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         }\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/taker/model.py:378\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, text, num, max_length, input_ids, do_sample, temperature, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50256\u001b[39m\n\u001b[1;32m    377\u001b[0m new_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39mnum \u001b[38;5;28;01mif\u001b[39;00m max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m max_length\n\u001b[0;32m--> 378\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# return in format (before, after)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode( input_ids,\n\u001b[1;32m    384\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2994\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2992\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[1;32m   2993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[0;32m-> 2994\u001b[0m     next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_warper\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   2997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/transformers/generation/logits_process.py:526\u001b[0m, in \u001b[0;36mTopKLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    524\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# Remove all tokens with a probability less than the last token of the top-k\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m<\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    527\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(indices_to_remove, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores_processed\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_new_tokens = 200\n",
    "temperature = 0.01\n",
    "\n",
    "# # test on single output\n",
    "# output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "# print(output[1])\n",
    "\n",
    "with HiddenPrints():\n",
    "    for i in range(20):\n",
    "        output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "\n",
    "        data = {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"model\": model_name,\n",
    "            \"transplant_layers\": (0, 15),\n",
    "            \"transferred_token_num\": n_tokens_to_transfer,\n",
    "            \"orig_prompt\": prompt_original,\n",
    "            \"transplant_prompt\": prompt_new,\n",
    "            \"output\": output[1],\n",
    "            \"other_info\": \"transfer-first-half-layers-alternative-loop--prompt-new-is-just-dots\",\n",
    "        }\n",
    "\n",
    "        with open(filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
