{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from taker import Model\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'google/gemma-2-2b-it' with int4:\n",
      "- Added 416 hooks across 26 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "m = Model(model_name, dtype=\"int4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - n_layers : 26\n",
      " - d_model  : 2304\n",
      " - n_heads  : 8\n",
      " - d_head   : 256\n",
      " - d_mlp    : 9216\n"
     ]
    }
   ],
   "source": [
    "m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 235265, 109]\n",
      "['<bos>', '.', '\\n\\n']\n",
      "[2, 20742, 90641, 235265, 109]\n",
      "['<bos>', 'Sunday', '▁dinners', '.', '\\n\\n']\n",
      "[2, 5519, 5519, 5519, 5519, 5519]\n",
      "['<bos>', '................', '................', '................', '................', '................']\n",
      "[2, 5519, 5519, 5519, 5519, 2779, 25984]\n",
      "['<bos>', '................', '................', '................', '................', '........', '.........']\n"
     ]
    }
   ],
   "source": [
    "idlist = m.get_ids(\".\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"Sunday dinners.\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\".\" * 16 * 5).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\".\" * (16 * 5 + 1)).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sunday dinners.\\n\\n',\n",
       " 'A simple phrase that conjures images of a warm, inviting home, the comforting aroma of roasted chicken and vegetables, and the')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate(\"Sunday dinners.\\n\\n\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short blog post about a recipe and the inspiration behind it.\n",
      " Do not include a title.\n",
      " Only reveal the dish after the story.\n",
      " Start with short story and then move to the recipe.\n",
      " To re-iterate, do not include a title.\n",
      "\n",
      " Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
      "\n",
      "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
      "\n",
      "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a short blog post about a recipe and the inspiration behind it.\n",
    " Do not include a title.\n",
    " Only reveal the dish after the story.\n",
    " Start with short story and then move to the recipe.\n",
    " To re-iterate, do not include a title.\"\"\"\n",
    "\n",
    "\n",
    "story = \"\"\"\n",
    "\\n Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
    "\n",
    "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
    "\n",
    "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\\n\\n\"\"\"\n",
    "\n",
    "prompt_original = prompt + story\n",
    "print(prompt_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_new='\\n\\n'\n",
      "'\\n\\n' '\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_transfer = 1\n",
    "\n",
    "idlist_original = m.get_ids(prompt_original).squeeze().tolist()\n",
    "tokens_original = m.tokenizer.convert_ids_to_tokens(idlist_original)\n",
    "n_tokens_original = len(tokens_original)\n",
    "\n",
    "# # prompt is just string of dots.\n",
    "# prompt_new = \".\" * 16 * n_tokens_to_transfer\n",
    "\n",
    "prefix = \"\"\n",
    "tokens_to_transfer = tokens_original[-n_tokens_to_transfer:]\n",
    "string_to_transfer = m.tokenizer.convert_tokens_to_string(tokens_to_transfer)\n",
    "prompt_new = prefix + string_to_transfer\n",
    "print(f\"{prompt_new=}\")\n",
    "\n",
    "idlist_new = m.get_ids(prompt_new).squeeze().tolist()\n",
    "tokens_new = m.tokenizer.convert_ids_to_tokens(idlist_new)\n",
    "n_tokens_new = len(tokens_new)\n",
    "\n",
    "token_index_map = {\n",
    "    n_tokens_original\n",
    "    - n_tokens_to_transfer\n",
    "    + i: n_tokens_new\n",
    "    - n_tokens_to_transfer\n",
    "    + i\n",
    "    for i in range(n_tokens_to_transfer)\n",
    "}\n",
    "\n",
    "# do sense check\n",
    "for index_original, index_new in token_index_map.items():\n",
    "    print(repr(tokens_original[index_original]), repr(tokens_new[index_new]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_0_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_1_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_1_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_2_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_2_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_3_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_3_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_4_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_4_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_5_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_5_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_6_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_6_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_7_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_7_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_8_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_8_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_9_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_9_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_10_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_10_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_11_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_11_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_12_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_12_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_13_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_13_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_14_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_14_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_15_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_15_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_16_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_16_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_17_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_17_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_18_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_18_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_19_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_19_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_20_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_20_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_21_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_21_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_22_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_22_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_23_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_23_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_24_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_24_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_25_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_25_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "for h in m.hooks.neuron_replace.values():\n",
    "    h.reset()\n",
    "\n",
    "print(m.hooks.neuron_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_0_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_1_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_1_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_2_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_2_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_3_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_3_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_4_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_4_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_5_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_5_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_6_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_6_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_7_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_7_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_8_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_8_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_9_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_9_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_10_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_10_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_11_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_11_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_12_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_12_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_13_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_13_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_14_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_14_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_15_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_15_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_16_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_16_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_17_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_17_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_18_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_18_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_19_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_19_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_20_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_20_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_21_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_21_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_22_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_22_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_23_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_23_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_24_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_24_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_25_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_25_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "activations_original = m.get_midlayer_activations(prompt_original)\n",
    "\n",
    "for original_index, new_index in token_index_map.items():\n",
    "    for layer_type in [\"mlp\", \"attn\"]:\n",
    "        # for layer_type in [\"attn\"]:\n",
    "        for layer_number in range(26):\n",
    "            hook = m.hooks.neuron_replace[f\"layer_{layer_number}_{layer_type}_pre_out\"]\n",
    "            hook.add_token(\n",
    "                new_index,\n",
    "                activations_original[layer_type][0, layer_number, original_index],\n",
    "            )\n",
    "\n",
    "# for original_index, new_index in token_index_map.items():\n",
    "#     for name, hook in m.hooks.neuron_replace.items():\n",
    "#         # name is of the form \"layer_{layer_number}_{layer_type}_pre_out\"\n",
    "#         _, layer_number, layer_type, _, _ = name.split(\"_\")\n",
    "#         layer_number = int(layer_number)\n",
    "#         hook.add_token(new_index, activations_original[layer_type][0, layer_number, original_index])\n",
    "\n",
    "print(m.hooks.neuron_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "current_time = \"2024-08-20_08-26-41\"\n",
    "filename = f\"../results/{current_time}_agnes_multi_token_transfer_LA_tests.jsonl\"\n",
    "\n",
    "if not exists(filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recipe for this dish is a secret, but the ingredients are simple and readily available. The dish is best served hot and fresh.\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 1 lb. boneless, skinless chicken breasts\n",
      "* 1/2 cup chopped onion\n",
      "* 1/2 cup chopped bell pepper\n",
      "* 1/4 cup chicken broth\n",
      "* 1/4 cup dry white wine\n",
      "* 1 tablespoon olive oil\n",
      "* Salt and pepper to taste\n",
      "* 1/2 cup chopped fresh parsley\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Heat olive oil in a large skillet over medium heat.\n",
      "2. Add chicken breasts and cook until browned on both sides.\n",
      "3. Add onion and bell pepper to the skillet and cook until softened.\n",
      "4. Stir in chicken broth, white wine, salt, and pepper.\n",
      "5. Bring the mixture to a boil, then reduce heat and simmer for 15 minutes.\n",
      "6. Stir in parsley and serve hot.\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 200\n",
    "temperature = 0.01\n",
    "\n",
    "# test on single output\n",
    "output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "print(output[1])\n",
    "\n",
    "# with HiddenPrints():\n",
    "#     for i in range(20):\n",
    "#         output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "\n",
    "#         data = {\n",
    "#             \"temperature\": temperature,\n",
    "#             \"max_new_tokens\": max_new_tokens,\n",
    "#             \"model\": model_name,\n",
    "#             \"transplant_layers\": (0, 25),\n",
    "#             \"transferred_token_num\": n_tokens_to_transfer,\n",
    "#             \"orig_prompt\": prompt_original,\n",
    "#             \"transplant_prompt\": prompt_new,\n",
    "#             \"output\": output[1],\n",
    "#             \"other_info\": \"try-different-transfer-layers-with-gemma\",\n",
    "#         }\n",
    "\n",
    "#         with open(filename, \"a\") as file:\n",
    "#             file.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_layers_transferred in range(1, 26, 2):\n",
    "    # RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "    for h in m.hooks.neuron_replace.values():\n",
    "        h.reset()\n",
    "\n",
    "    activations_original = m.get_midlayer_activations(prompt_original)\n",
    "\n",
    "    for original_index, new_index in token_index_map.items():\n",
    "        for layer_type in [\"mlp\", \"attn\"]:\n",
    "            # for layer_type in [\"attn\"]:\n",
    "            for layer_number in range(n_layers_transferred):\n",
    "                hook = m.hooks.neuron_replace[\n",
    "                    f\"layer_{layer_number}_{layer_type}_pre_out\"\n",
    "                ]\n",
    "                hook.add_token(\n",
    "                    new_index,\n",
    "                    activations_original[layer_type][0, layer_number, original_index],\n",
    "                )\n",
    "\n",
    "    max_new_tokens = 100\n",
    "    temperature = 0.01\n",
    "\n",
    "    with HiddenPrints():\n",
    "        for i in range(3):\n",
    "            output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "\n",
    "            data = {\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"model\": model_name,\n",
    "                \"transplant_layers\": (0, n_layers_transferred - 1),\n",
    "                \"transferred_token_num\": n_tokens_to_transfer,\n",
    "                \"orig_prompt\": prompt_original,\n",
    "                \"transplant_prompt\": prompt_new,\n",
    "                \"other_info\": f\"transfer-first-{n_layers_transferred}-layers\",\n",
    "                \"output\": output[1],\n",
    "            }\n",
    "\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
