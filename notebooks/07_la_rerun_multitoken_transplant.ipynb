{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from taker import Model\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os.path import exists\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'microsoft/phi-3-mini-4k-instruct' with int4:\n",
      "- Added 512 hooks across 32 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "m = Model(model_name, dtype=\"int4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - n_layers : 32\n",
      " - d_model  : 3072\n",
      " - n_heads  : 32\n",
      " - d_head   : 96\n",
      " - d_mlp    : 8192\n"
     ]
    }
   ],
   "source": [
    "m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[869, 13, 13]\n",
      "['▁.', '<0x0A>', '<0x0A>']\n",
      "[16340, 270, 16697, 29889, 13, 13]\n",
      "['▁Sunday', '▁d', 'inners', '.', '<0x0A>', '<0x0A>']\n"
     ]
    }
   ],
   "source": [
    "idlist = m.get_ids(\".\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"Sunday dinners.\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short blog post about a recipe and the inspiration behind it.\n",
      " Do not include a title.\n",
      " Only reveal the dish after the story.\n",
      " Start with short story and then move to the recipe.\n",
      " To re-iterate, do not include a title.\n",
      "\n",
      " Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
      "\n",
      "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
      "\n",
      "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a short blog post about a recipe and the inspiration behind it.\n",
    " Do not include a title.\n",
    " Only reveal the dish after the story.\n",
    " Start with short story and then move to the recipe.\n",
    " To re-iterate, do not include a title.\"\"\"\n",
    "\n",
    "\n",
    "story = \"\"\"\n",
    "\\n Once upon a time, in a quaint little village nestled between rolling hills and verdant fields, there lived an elderly woman named Agnes. Agnes was known for her warm smile and her legendary Sunday dinners that brought the entire neighborhood together. Her recipes were family heirlooms, passed down through generations, with each family adding their own touch to the final dish.\n",
    "\n",
    "One crisp autumn evening, Agnes was reminiscing about her childhood, and how her grandmother used to gather everyone around the dinner table, sharing stories and laughter. These were the moments that shaped her, the memories that she passed on to her own children and grandchildren.\n",
    "\n",
    "Inspired by her grandmother's legacy, Agnes decided to create a new dish that would encapsulate the essence of those cherished gatherings. She wanted something that was comforting and nourishing, a dish that could be prepared with love and shared with others. After days of experimentation, she finally created a recipe that she believed truly captured the spirit of her family's Sunday dinners.\\n\\n\"\"\"\n",
    "\n",
    "prompt_original = prompt + story\n",
    "print(prompt_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_new='\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_transfer = 2\n",
    "\n",
    "idlist_original = m.get_ids(prompt_original).squeeze().tolist()\n",
    "tokens_original = m.tokenizer.convert_ids_to_tokens(idlist_original)\n",
    "n_tokens_original = len(tokens_original)\n",
    "tokens_to_transfer = tokens_original[-n_tokens_to_transfer:]\n",
    "string_to_transfer = m.tokenizer.convert_tokens_to_string(tokens_to_transfer)\n",
    "\n",
    "# # dummy prompt for testing\n",
    "# prompt_new = '................'*400\n",
    "\n",
    "prefix = \"\"\n",
    "prompt_new = prefix + string_to_transfer\n",
    "print(f\"{prompt_new=}\")\n",
    "idlist_new = m.get_ids(prompt_new).squeeze().tolist()\n",
    "tokens_new = m.tokenizer.convert_ids_to_tokens(idlist_new)\n",
    "n_tokens_new = len(tokens_new)\n",
    "\n",
    "token_index_map = {\n",
    "    n_tokens_original - n_tokens_to_transfer + i: n_tokens_new - n_tokens_to_transfer + i\n",
    "    for i in range(n_tokens_to_transfer)\n",
    "}\n",
    "\n",
    "# do sense check\n",
    "for index_original, index_new in token_index_map.items():\n",
    "    assert tokens_original[index_original] == tokens_new[index_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0_attn_pre_out': NeuronReplace(), 'layer_0_mlp_pre_out': NeuronReplace(), 'layer_1_attn_pre_out': NeuronReplace(), 'layer_1_mlp_pre_out': NeuronReplace(), 'layer_2_attn_pre_out': NeuronReplace(), 'layer_2_mlp_pre_out': NeuronReplace(), 'layer_3_attn_pre_out': NeuronReplace(), 'layer_3_mlp_pre_out': NeuronReplace(), 'layer_4_attn_pre_out': NeuronReplace(), 'layer_4_mlp_pre_out': NeuronReplace(), 'layer_5_attn_pre_out': NeuronReplace(), 'layer_5_mlp_pre_out': NeuronReplace(), 'layer_6_attn_pre_out': NeuronReplace(), 'layer_6_mlp_pre_out': NeuronReplace(), 'layer_7_attn_pre_out': NeuronReplace(), 'layer_7_mlp_pre_out': NeuronReplace(), 'layer_8_attn_pre_out': NeuronReplace(), 'layer_8_mlp_pre_out': NeuronReplace(), 'layer_9_attn_pre_out': NeuronReplace(), 'layer_9_mlp_pre_out': NeuronReplace(), 'layer_10_attn_pre_out': NeuronReplace(), 'layer_10_mlp_pre_out': NeuronReplace(), 'layer_11_attn_pre_out': NeuronReplace(), 'layer_11_mlp_pre_out': NeuronReplace(), 'layer_12_attn_pre_out': NeuronReplace(), 'layer_12_mlp_pre_out': NeuronReplace(), 'layer_13_attn_pre_out': NeuronReplace(), 'layer_13_mlp_pre_out': NeuronReplace(), 'layer_14_attn_pre_out': NeuronReplace(), 'layer_14_mlp_pre_out': NeuronReplace(), 'layer_15_attn_pre_out': NeuronReplace(), 'layer_15_mlp_pre_out': NeuronReplace(), 'layer_16_attn_pre_out': NeuronReplace(), 'layer_16_mlp_pre_out': NeuronReplace(), 'layer_17_attn_pre_out': NeuronReplace(), 'layer_17_mlp_pre_out': NeuronReplace(), 'layer_18_attn_pre_out': NeuronReplace(), 'layer_18_mlp_pre_out': NeuronReplace(), 'layer_19_attn_pre_out': NeuronReplace(), 'layer_19_mlp_pre_out': NeuronReplace(), 'layer_20_attn_pre_out': NeuronReplace(), 'layer_20_mlp_pre_out': NeuronReplace(), 'layer_21_attn_pre_out': NeuronReplace(), 'layer_21_mlp_pre_out': NeuronReplace(), 'layer_22_attn_pre_out': NeuronReplace(), 'layer_22_mlp_pre_out': NeuronReplace(), 'layer_23_attn_pre_out': NeuronReplace(), 'layer_23_mlp_pre_out': NeuronReplace(), 'layer_24_attn_pre_out': NeuronReplace(), 'layer_24_mlp_pre_out': NeuronReplace(), 'layer_25_attn_pre_out': NeuronReplace(), 'layer_25_mlp_pre_out': NeuronReplace(), 'layer_26_attn_pre_out': NeuronReplace(), 'layer_26_mlp_pre_out': NeuronReplace(), 'layer_27_attn_pre_out': NeuronReplace(), 'layer_27_mlp_pre_out': NeuronReplace(), 'layer_28_attn_pre_out': NeuronReplace(), 'layer_28_mlp_pre_out': NeuronReplace(), 'layer_29_attn_pre_out': NeuronReplace(), 'layer_29_mlp_pre_out': NeuronReplace(), 'layer_30_attn_pre_out': NeuronReplace(), 'layer_30_mlp_pre_out': NeuronReplace(), 'layer_31_attn_pre_out': NeuronReplace(), 'layer_31_mlp_pre_out': NeuronReplace()}\n"
     ]
    }
   ],
   "source": [
    "# RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "for h in m.hooks.neuron_replace.values():\n",
    "    h.reset()\n",
    "\n",
    "print(m.hooks.neuron_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_original = m.get_midlayer_activations(prompt_original)\n",
    "\n",
    "for original_index, new_index in token_index_map.items():\n",
    "    for name, hook in m.hooks.neuron_replace.items():\n",
    "        # name is of the form \"layer_{layer_number}_{layer_type}_pre_out\"\n",
    "        _, layer_number, layer_type, _, _ = name.split(\"_\")\n",
    "        layer_number = int(layer_number)\n",
    "        hook.add_token(new_index, activations_original[layer_type][0, layer_number, original_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"../results/{current_time}_agnes_multi_token_transfer_LA.jsonl\"\n",
    "\n",
    "if not exists(filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 200\n",
    "temperature = 0.2\n",
    "\n",
    "# # test on single output\n",
    "# output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "# print(output[1])\n",
    "\n",
    "with HiddenPrints():\n",
    "    for i in range(20):\n",
    "        output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "        \n",
    "        data = {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"model\": model_name,\n",
    "            \"transplant_layers\": (0,m.cfg.n_layers + 1),\n",
    "            \"transferred_token_num\": n_tokens_to_transfer,\n",
    "            \"orig_prompt\": prompt_original,\n",
    "            \"transplant_prompt\": prompt_new,\n",
    "            \"output\": output[1],\n",
    "        }\n",
    "\n",
    "        with open(filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
