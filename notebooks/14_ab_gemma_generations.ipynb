{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "from taker import Model\n",
    "from taker.hooks import HookConfig\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'google/gemma-2-9b-it' with bfp16:\n",
      "- Added 672 hooks across 42 layers\n",
      " - n_layers : 42\n",
      " - d_model  : 3584\n",
      " - n_heads  : 16\n",
      " - d_head   : 256\n",
      " - d_mlp    : 14336\n"
     ]
    }
   ],
   "source": [
    "m = Model(\"google/gemma-2-9b-it\", dtype=\"bfp16\")\n",
    "# m = Model(\"google/gemma-2-2b-it\")\n",
    "m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../promptsV1.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    readdata = list(reader)\n",
    "    readdata = readdata[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tell me about a weekend in a mountain cabin in 150 words and then tell me about the Saint Louis Arch in another 150 words. Start the second paragraph with \"On a lovely Sunday morning the roadways are bustling\"\\n\\nOn a crisp autumn weekend, I found myself nestled in a cozy mountain cabin. The air was crisp and clean, the leaves were ablaze in fiery hues of red and gold, and the silence was broken only by the crackling fire in the stone fireplace. I spent my time reading by the window, sipping hot cocoa, and watching the snow softly fall. The cabin was filled with the scent of pine needles and woodsmoke, and the warmth of the fire radiated through the room. It was a perfect escape from the hustle and bustle of city life.\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# filename = f\"../gemma_results/latest_orig_generation.jsonl\"\n",
    "# if not exists(filename):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         pass\n",
    "\n",
    "# max_new_tokens = 200\n",
    "# temperature = 0.3\n",
    "\n",
    "# [h.reset() for h in m.hooks.neuron_replace.values()] #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "# for prompt in readdata:\n",
    "#     prompt = prompt[0]\n",
    "#     with HiddenPrints():\n",
    "#         for i in range(10):\n",
    "#             output = m.generate(prompt, max_new_tokens, temperature=temperature)\n",
    "#             print(output)\n",
    "#             data = {\n",
    "#                 \"temperature\": temperature,\n",
    "#                 \"max_new_tokens\": max_new_tokens,\n",
    "#                 \"model\": \"google/gemma-2-2b-it\",\n",
    "#                 \"type\": \"original\",\n",
    "#                 \"transplant_layers\": None,\n",
    "#                 \"prompt\": prompt,\n",
    "#                 \"output\": output[1],\n",
    "#             }\n",
    "\n",
    "#             with open(filename, \"a\") as file:\n",
    "#                 file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "# filename = f\"../gemma_results/latest_neutral_generation_V2.jsonl\"\n",
    "# if not exists(filename):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         pass\n",
    "\n",
    "# neutral_prompts = [\"\\n\\n\"]\n",
    "# # for neutral in neutral_prompts:                \n",
    "# #     with HiddenPrints():\n",
    "# #         for i in range(50):\n",
    "# #             output = m.generate(neutral, max_new_tokens, temperature=temperature)\n",
    "            \n",
    "# #             data = {\n",
    "# #                 \"temperature\": temperature,\n",
    "# #                 \"max_new_tokens\": max_new_tokens,\n",
    "# #                 \"model\": \"google/gemma-2-2b-it\",\n",
    "# #                 \"type\": \"neutral\",\n",
    "# #                 \"transplant_layers\": None,\n",
    "# #                 \"prompt\": neutral,\n",
    "# #                 \"output\": output[1],\n",
    "# #             }\n",
    "\n",
    "# #             with open(filename, \"a\") as file:\n",
    "# #                 file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# orig_df = pd.read_json(f\"../gemma_results/latest_orig_generation.jsonl\", lines=True)\n",
    "# def split_at_double_newline(text):\n",
    "#     # Ensure we are only working with strings longer than 15 characters\n",
    "#     if len(text) > 15:\n",
    "#         # Search for the first double newline after the 15th character\n",
    "#         pos = text.find('\\n\\n', 15)\n",
    "#         if pos != -1:  # Check if double newline was found\n",
    "#             return text[:pos+2], text[pos:]  # Split and remove the newline from the second part\n",
    "#     return text, None  # If no split is required, return the original text and None\n",
    "\n",
    "# # Apply the function to the DataFrame column\n",
    "# orig_df['paragraph1'], orig_df['paragraph2'] = zip(*orig_df['output'].apply(split_at_double_newline))\n",
    "# orig_df['paragraph1'] = orig_df['prompt'].astype(str) + orig_df['paragraph1'].astype(str)\n",
    "# print(repr(orig_df['paragraph1'][0]))\n",
    "# filename = f\"../gemma_results/latest_transferred_generation_V2.jsonl\"\n",
    "# if not exists(filename):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         pass\n",
    "    \n",
    "# for info_prompt in orig_df['paragraph1']:\n",
    "#     acts = m.get_midlayer_activations(info_prompt)\n",
    "#     orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "#     for neutral_prompt in neutral_prompts:\n",
    "#         new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1        \n",
    "\n",
    "#         [h.reset() for h in m.hooks.neuron_replace.values()] #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "#         for layer_index in range(0,26):\n",
    "#             m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, acts[\"mlp\"][0, layer_index, orig_token_index]*100)\n",
    "#             m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, acts[\"attn\"][0, layer_index, orig_token_index]*100)\n",
    "#         with HiddenPrints():\n",
    "#             # for i in range(1):\n",
    "#                 output = m.generate(neutral_prompt, max_new_tokens, temperature=temperature)\n",
    "                \n",
    "#                 data = {\n",
    "#                     \"temperature\": temperature,\n",
    "#                     \"max_new_tokens\": max_new_tokens,\n",
    "#                     \"model\": \"google/gemma-2-2b-it\",\n",
    "#                     \"type\": \"transferred\",\n",
    "#                     \"num_transferred_tokens\": 2,\n",
    "#                     \"transplant_layers\": (0,26),\n",
    "#                     \"orig_prompt\": info_prompt,\n",
    "#                     \"transplant_prompt\": neutral_prompt,\n",
    "#                     \"output\": output[1],\n",
    "#                 }\n",
    "\n",
    "#                 with open(filename, \"a\") as file:\n",
    "#                     file.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 with Cheating Neutral Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tell me about a weekend in a mountain cabin in 150 words and then tell me about disconnecting from technology in another 150 words. Only do that. Make sure you don’t add any headings or comments.\\n\\nThe scent of pine needles filled the air as we drove up the winding mountain road.  Our cozy cabin, nestled amongst towering trees, welcomed us with warmth and the promise of a peaceful escape.  Days were spent hiking through sun-dappled forests, the sound of birdsong our only soundtrack. Evenings were spent by the crackling fireplace, sharing stories and laughter. The stars, unfiltered by city lights, blazed across the night sky, a breathtaking spectacle.  \\n\\n'\n",
      "'\\n\\nThe world seemed to fade away as I stepped away from the constant buzz of notifications and the endless scroll.  Silence, once a rarity, became a comforting presence.  I rediscovered the joy of reading a physical book, the pages turning slowly, each word savored.  Conversations flowed more deeply, unhurried and genuine.  Nature, with its gentle rhythms, became my guide, reminding me of the simple beauty of being present. \\n\\n\\n'\n",
      "\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "The world\n",
      "\n",
      "\n",
      "The world seemed\n",
      "\n",
      "\n",
      "The world seemed to\n",
      "\n",
      "\n",
      "The world seemed to fade\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neutral \u001b[38;5;129;01min\u001b[39;00m orig_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral_prompt\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneutralnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]:                \n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m HiddenPrints():\n\u001b[0;32m---> 67\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_new_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     78\u001b[0m         }\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/taker/model.py:394\u001b[0m, in \u001b[0;36mModel.generate\u001b[0;34m(self, text, num, max_length, input_ids, inputs_embeds, do_sample, temperature, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Generate from input_ids if available, seems to work more reliably (eg gemma2)\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     generate_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     text_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode( generate_ids[:, \u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m]):],\n\u001b[1;32m    398\u001b[0m         skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_before, text_after\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:1087\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1073\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1074\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1075\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1084\u001b[0m )\n\u001b[1;32m   1086\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1087\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfinal_logit_softcapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfinal_logit_softcapping\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/SPAR/interp-ab/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = f\"../gemma9b_results/latest_orig_generation_new.jsonl\"\n",
    "if not exists(filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "max_new_tokens = 200\n",
    "temperature = 0.3\n",
    "\n",
    "# [h.reset() for h in m.hooks.neuron_replace.values()] #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "# for prompt in readdata:\n",
    "#     prompt = prompt[0]\n",
    "#     with HiddenPrints():\n",
    "#         for i in range(50):\n",
    "#             output = m.generate(prompt, max_new_tokens, temperature=temperature)\n",
    "#             print(output)\n",
    "#             data = {\n",
    "#                 \"temperature\": temperature,\n",
    "#                 \"max_new_tokens\": max_new_tokens,\n",
    "#                 \"model\": \"google/gemma-2-9b-it\",\n",
    "#                 \"type\": \"original\",\n",
    "#                 \"transplant_layers\": None,\n",
    "#                 \"prompt\": prompt,\n",
    "#                 \"output\": output[1],\n",
    "#             }\n",
    "\n",
    "#             with open(filename, \"a\") as file:\n",
    "#                 file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "orig_df = pd.read_json(f\"../gemma9b_results/latest_orig_generation_new.jsonl\", lines=True)\n",
    "def split_at_double_newline(text):\n",
    "    # Ensure we are only working with strings longer than 15 characters\n",
    "    if len(text) > 15:\n",
    "        # Search for the first double newline after the 15th character\n",
    "        pos = text.find('\\n\\n', 15)\n",
    "        if pos != -1:  # Check if double newline was found\n",
    "            return text[:pos+2], text[pos:]  # Split and remove the newline from the second part\n",
    "    return text, text  # If no split is required, return the original text and None\n",
    "\n",
    "orig_df['paragraph1'], orig_df['paragraph2'] = zip(*orig_df['output'].apply(split_at_double_newline))\n",
    "orig_df['paragraph1'] = orig_df['prompt'].astype(str) + orig_df['paragraph1'].astype(str)\n",
    "print(repr(orig_df['paragraph1'][0]))\n",
    "def get_neutral_prompt(text):\n",
    "    idlist = m.get_ids(text).squeeze().tolist()\n",
    "    neutral_tokens = m.tokenizer.convert_ids_to_tokens(idlist)\n",
    "    neutral_tokens = [entry.replace(\"▁\", \" \") for entry in neutral_tokens]\n",
    "    sep = ''\n",
    "    return sep.join(neutral_tokens[1:3]), sep.join(neutral_tokens[1:4]), sep.join(neutral_tokens[1:5]), sep.join(neutral_tokens[1:6]), sep.join(neutral_tokens[1:7])\n",
    "print(repr(orig_df['paragraph2'][0]))\n",
    "orig_df[\"neutral_prompt1\"], orig_df[\"neutral_prompt2\"], orig_df[\"neutral_prompt3\"], orig_df[\"neutral_prompt4\"], orig_df[\"neutral_prompt5\"] = zip(*orig_df['paragraph2'].apply(get_neutral_prompt))\n",
    "print(orig_df[\"neutral_prompt1\"][0])\n",
    "print(orig_df[\"neutral_prompt2\"][0])\n",
    "print(orig_df[\"neutral_prompt3\"][0])\n",
    "print(orig_df[\"neutral_prompt4\"][0])\n",
    "print(orig_df[\"neutral_prompt5\"][0])\n",
    "\n",
    "[h.reset() for h in m.hooks.neuron_replace.values()] #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "\n",
    "\n",
    "for neutralnum in range(1, 6):\n",
    "    filename = f\"../gemma9b_results/latest_neutral{neutralnum}_generation.jsonl\"\n",
    "    if not exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            pass\n",
    "\n",
    "    for neutral in orig_df[f\"neutral_prompt{neutralnum}\"]:                \n",
    "        with HiddenPrints():\n",
    "            output = m.generate(neutral, max_new_tokens, temperature=temperature)\n",
    "            \n",
    "            data = {\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"model\": \"google/gemma-2-9b-it\",\n",
    "                \"type\": \"neutral\",\n",
    "                \"cheat_tokens\": neutralnum,\n",
    "                \"transplant_layers\": None,\n",
    "                \"prompt\": neutral,\n",
    "                \"output\": output[1],\n",
    "            }\n",
    "\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "neutral_prompts = [\"\\n\\n\"]\n",
    "# Apply the function to the DataFrame column\n",
    "filename = f\"../gemma9b_results/latest_transferred_generation.jsonl\"\n",
    "if not exists(filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pass\n",
    "    \n",
    "for info_prompt in orig_df['paragraph1']:\n",
    "    acts = m.get_midlayer_activations(info_prompt)\n",
    "    orig_token_index = m.get_ids(info_prompt).shape[1] - 1\n",
    "    for neutral_prompt in neutral_prompts:\n",
    "        new_token_index  = m.get_ids(neutral_prompt).shape[1] - 1        \n",
    "\n",
    "        [h.reset() for h in m.hooks.neuron_replace.values()] #RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "        for layer_index in range(0,26):\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_mlp_pre_out\"].add_token(new_token_index, acts[\"mlp\"][0, layer_index, orig_token_index])\n",
    "            m.hooks.neuron_replace[f\"layer_{layer_index}_attn_pre_out\"].add_token(new_token_index, acts[\"attn\"][0, layer_index, orig_token_index])\n",
    "        with HiddenPrints():\n",
    "            # for i in range(1):\n",
    "                output = m.generate(neutral_prompt, max_new_tokens, temperature=temperature)\n",
    "                \n",
    "                data = {\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_new_tokens\": max_new_tokens,\n",
    "                    \"model\": \"google/gemma-2-9b-it\",\n",
    "                    \"type\": \"transferred\",\n",
    "                    \"num_transferred_tokens\": 1,\n",
    "                    \"transplant_layers\": (0,26),\n",
    "                    \"orig_prompt\": info_prompt,\n",
    "                    \"transplant_prompt\": neutral_prompt,\n",
    "                    \"output\": output[1],\n",
    "                }\n",
    "\n",
    "                with open(filename, \"a\") as file:\n",
    "                    file.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
