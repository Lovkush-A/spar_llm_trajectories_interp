{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring activations from end of prompt and generating text\n",
    "\n",
    "Trying on other scenarios.\n",
    "\n",
    "Contents:\n",
    "- Create helper function\n",
    "- Load model\n",
    "- Create prompts\n",
    "- Transfer activations\n",
    "- Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from taker import Model\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'google/gemma-2-2b-it' with int4:\n",
      "- Added 416 hooks across 26 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SPAR/interp-la/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "m = Model(model_name, dtype=\"int4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - n_layers : 26\n",
      " - d_model  : 2304\n",
      " - n_heads  : 8\n",
      " - d_head   : 256\n",
      " - d_mlp    : 9216\n"
     ]
    }
   ],
   "source": [
    "m.show_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 235265, 109]\n",
      "['<bos>', '.', '\\n\\n']\n",
      "[2, 20742, 90641, 235265, 109]\n",
      "['<bos>', 'Sunday', '▁dinners', '.', '\\n\\n']\n",
      "[2, 5519, 5519, 5519, 5519, 5519]\n",
      "['<bos>', '................', '................', '................', '................', '................']\n",
      "[2, 5519, 5519, 5519, 5519, 2779, 25984]\n",
      "['<bos>', '................', '................', '................', '................', '........', '.........']\n",
      "[2, 177176, 177176, 177176, 177176, 177176]\n",
      "['<bos>', '@@@@@@@@', '@@@@@@@@', '@@@@@@@@', '@@@@@@@@', '@@@@@@@@']\n",
      "[2, 177176, 177176, 177176, 177176, 177176, 235348]\n",
      "['<bos>', '@@@@@@@@', '@@@@@@@@', '@@@@@@@@', '@@@@@@@@', '@@@@@@@@', '@']\n",
      "[2, 3755, 3755, 3755, 3755, 3755]\n",
      "['<bos>', '----------------', '----------------', '----------------', '----------------', '----------------']\n",
      "[2, 3755, 3755, 3755, 3755, 3755, 235290]\n",
      "['<bos>', '----------------', '----------------', '----------------', '----------------', '----------------', '-']\n"
     ]
    }
   ],
   "source": [
    "idlist = m.get_ids(\".\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"Sunday dinners.\\n\\n\").squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "# # for phi3\n",
    "# idlist = m.get_ids(\".\" * 16 * 5).squeeze().tolist()\n",
    "# print(idlist)\n",
    "# print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "# idlist = m.get_ids(\".\" * (16 * 5 + 1)).squeeze().tolist()\n",
    "# print(idlist)\n",
    "# print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "# for gemma\n",
    "idlist = m.get_ids(\".\" * 16 * 5).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\".\" * (16 * 5 + 1)).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"@\" * 8 * 5).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"@\" * (8 * 5 + 1)).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"-\" * 16 * 5).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))\n",
    "\n",
    "idlist = m.get_ids(\"-\" * (16 * 5 + 1)).squeeze().tolist()\n",
    "print(idlist)\n",
    "print(m.tokenizer.convert_ids_to_tokens(idlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The thrill of axe-throwing is amplified when it's done in a competition. The sport is both physically demanding and mentally engaging, demanding precision and focus to hit targets with accuracy. The feeling of adrenaline rushes as the axe flies through the air, and the anticipation of the next toss further fuels the excitement. The game of axe-throwing can be enjoyed at home or at a dedicated venue, catering to all skill levels from amateur to professional. \\n\\nIn the world of Chinese lore, dragons, phoenixes, and other majestic creatures hold significant cultural and spiritual significance. They often serve as powerful symbols, embodying qualities like wisdom, power, and harmony. These mythical beasts are deeply ingrained in Chinese culture and tradition, their presence deeply felt in their art, literature, and mythology. \\n\"\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Tell me about {topic1} in 150 words and then tell me about {topic2} in another 150 words. Only do that. Make sure you don't add any headings or comments.\\n\\n\"\n",
    "# prompt_template = \"Tell me about {topic1} in 150 words and then tell me about {topic2} in another 150 words and then tell me about {topic3} in another 150 words. Only do that. Make sure you don't add any headings or comments.\\n\\n\"\n",
    "topic1 = \"axe-throwing\"\n",
    "topic2 = \"Chinese animals\"\n",
    "# topic3 = \"the history of paper\"\n",
    "prompt = prompt_template.format(topic1=topic1, topic2=topic2)\n",
    "# prompt = prompt_template.format(topic1=topic1, topic2=topic2, topic3=topic3)\n",
    "first_output = m.generate(prompt, 400)\n",
    "print(repr(first_output[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about axe-throwing in 150 words and then tell me about Chinese animals in another 150 words. Only do that. Make sure you don't add any headings or comments.\n",
      "\n",
      "The thrill of axe-throwing is amplified when it's done in a competition. The sport is both physically demanding and mentally engaging, demanding precision and focus to hit targets with accuracy. The feeling of adrenaline rushes as the axe flies through the air, and the anticipation of the next toss further fuels the excitement. The game of axe-throwing can be enjoyed at home or at a dedicated venue, catering to all skill levels from amateur to professional. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = first_output[1][:first_output[1].find('\\n\\n')+2]\n",
    "prompt_original = prompt + start\n",
    "print(prompt_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_prompt_from_end_tokens(m, prompt_original, n_tokens_to_transfer, prefix):\n",
    "    idlist_original = m.get_ids(prompt_original).squeeze().tolist()\n",
    "    tokens_original = m.tokenizer.convert_ids_to_tokens(idlist_original)\n",
    "    n_tokens_original = len(tokens_original)\n",
    "\n",
    "    tokens_to_transfer = tokens_original[-n_tokens_to_transfer:]\n",
    "    string_to_transfer = m.tokenizer.convert_tokens_to_string(tokens_to_transfer)\n",
    "    prompt_new = prefix + string_to_transfer\n",
    "\n",
    "    idlist_new = m.get_ids(prompt_new).squeeze().tolist()\n",
    "    tokens_new = m.tokenizer.convert_ids_to_tokens(idlist_new)\n",
    "    n_tokens_new = len(tokens_new)\n",
    "\n",
    "    token_index_map = {\n",
    "        n_tokens_original\n",
    "        - n_tokens_to_transfer\n",
    "        + i: n_tokens_new\n",
    "        - n_tokens_to_transfer\n",
    "        + i\n",
    "        for i in range(n_tokens_to_transfer)\n",
    "    }\n",
    "\n",
    "    return prompt_new, token_index_map, tokens_original, tokens_new\n",
    "\n",
    "\n",
    "def create_new_prompt_by_repeating_dummy_string(\n",
    "    m, prompt_original, dummy_string, n_tokens_to_transfer, prefix\n",
    "):\n",
    "    idlist_original = m.get_ids(prompt_original).squeeze().tolist()\n",
    "    tokens_original = m.tokenizer.convert_ids_to_tokens(idlist_original)\n",
    "    n_tokens_original = len(tokens_original)\n",
    "\n",
    "    prompt_new = prefix + dummy_string * n_tokens_to_transfer\n",
    "\n",
    "    idlist_new = m.get_ids(prompt_new).squeeze().tolist()\n",
    "    tokens_new = m.tokenizer.convert_ids_to_tokens(idlist_new)\n",
    "    n_tokens_new = len(tokens_new)\n",
    "\n",
    "    token_index_map = {\n",
    "        n_tokens_original\n",
    "        - n_tokens_to_transfer\n",
    "        + i: n_tokens_new\n",
    "        - n_tokens_to_transfer\n",
    "        + i\n",
    "        for i in range(n_tokens_to_transfer)\n",
    "    }\n",
    "\n",
    "    return prompt_new, token_index_map, tokens_original, tokens_new\n",
    "\n",
    "\n",
    "def create_new_prompt_by_transferring_all_of_one_token_type(\n",
    "    m, prompt_original, token_to_transfer, dummy_string\n",
    "):\n",
    "    idlist_original = m.get_ids(prompt_original).squeeze().tolist()\n",
    "    tokens_original = m.tokenizer.convert_ids_to_tokens(idlist_original)\n",
    "\n",
    "    prompt_new = \"\"\n",
    "    n_tokens_to_transfer = 0\n",
    "    # this maps the index of token in tokens_original to the index of token in tokens_new\n",
    "    token_index_map = {}\n",
    "\n",
    "    for i, token in enumerate(tokens_original):\n",
    "        if token == token_to_transfer:\n",
    "            prompt_new += dummy_string\n",
    "            n_tokens_to_transfer += 1\n",
    "            token_index_map[i] = n_tokens_to_transfer\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    idlist_new = m.get_ids(prompt_new).squeeze().tolist()\n",
    "    tokens_new = m.tokenizer.convert_ids_to_tokens(idlist_new)\n",
    "\n",
    "    return prompt_new, token_index_map, tokens_original, tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_new='\\n\\n'\n",
      "tokens_new=['<bos>', '\\n\\n']\n",
      "\n",
      "{135: 1}\n",
      "\n",
      "'\\n\\n' '\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_transfer = 1\n",
    "\n",
    "prompt_new, token_index_map, tokens_original, tokens_new = (\n",
    "    create_new_prompt_from_end_tokens(\n",
    "        m=m, prompt_original=prompt_original, n_tokens_to_transfer=n_tokens_to_transfer, prefix=\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# prompt_new, token_index_map, tokens_original, tokens_new = (\n",
    "#     create_new_prompt_by_repeating_dummy_string(\n",
    "#         m=m,\n",
    "#         prompt_original=prompt_original,\n",
    "#         dummy_string=\".\" * 16,\n",
    "#         n_tokens_to_transfer=n_tokens_to_transfer,\n",
    "#         prefix=\"\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# prompt_new, token_index_map, tokens_original, tokens_new = create_new_prompt_by_transferring_all_of_one_token_type(\n",
    "#     m=m, prompt_original=prompt_original, token_to_transfer=\"\\n\\n\", dummy_string=\"@\"*8)\n",
    "\n",
    "# do sense check\n",
    "print(f\"{prompt_new=}\")\n",
    "print(f\"{tokens_new=}\")\n",
    "print()\n",
    "print(token_index_map)\n",
    "print()\n",
    "for index_original, index_new in token_index_map.items():\n",
    "    print(repr(tokens_original[index_original]), repr(tokens_new[index_new]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_0_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_1_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_1_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_2_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_2_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_3_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_3_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_4_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_4_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_5_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_5_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_6_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_6_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_7_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_7_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_8_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_8_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_9_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_9_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_10_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_10_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_11_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_11_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_12_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_12_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_13_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_13_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_14_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_14_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_15_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_15_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_16_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_16_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_17_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_17_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_18_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_18_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_19_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_19_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_20_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_20_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_21_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_21_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_22_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_22_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_23_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_23_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_24_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_24_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_25_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      "), 'layer_25_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict()\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# RESET HOOKS BEFORE TRANSPLANTING NEXT SET OF ACTIVATIONS\n",
    "for h in m.hooks.neuron_replace.values():\n",
    "    h.reset()\n",
    "\n",
    "print(m.hooks.neuron_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_0_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_1_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_1_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_2_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_2_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_3_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_3_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_4_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_4_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_5_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_5_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_6_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_6_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_7_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_7_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_8_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_8_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_9_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_9_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_10_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_10_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_11_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_11_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_12_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_12_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_13_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_13_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_14_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_14_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_15_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_15_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_16_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_16_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_17_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_17_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_18_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_18_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_19_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_19_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_20_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_20_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_21_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_21_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_22_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_22_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_23_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_23_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_24_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_24_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      "), 'layer_25_attn_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 8x256 (cuda:0)])\n",
      "), 'layer_25_mlp_pre_out': NeuronReplace(\n",
      "  (param): ParameterDict(  (1): Parameter containing: [torch.cuda.HalfTensor of size 9216 (cuda:0)])\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "activations_original = m.get_midlayer_activations(prompt_original)\n",
    "\n",
    "for original_index, new_index in token_index_map.items():\n",
    "    for layer_type in [\"mlp\", \"attn\"]:\n",
    "        # for layer_type in [\"attn\"]:\n",
    "        for layer_number in range(m.cfg.n_layers):\n",
    "            hook = m.hooks.neuron_replace[f\"layer_{layer_number}_{layer_type}_pre_out\"]\n",
    "            hook.add_token(\n",
    "                new_index,\n",
    "                activations_original[layer_type][0, layer_number, original_index],\n",
    "            )\n",
    "\n",
    "print(m.hooks.neuron_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "current_time = \"2024-08-22_08-26-53\"\n",
    "filename = f\"../results/{current_time}_LA_activation_transfer_different_scenarios.jsonl\"\n",
    "\n",
    "if not exists(filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 150\n",
    "temperature = 0.2\n",
    "\n",
    "# # test on single output\n",
    "# output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "# print(repr(output[1]))\n",
    "\n",
    "with HiddenPrints():\n",
    "    for i in range(3):\n",
    "        output = m.generate(prompt_new, max_new_tokens, temperature=temperature)\n",
    "\n",
    "        data = {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"model\": model_name,\n",
    "            \"transplant_layers\": (0, m.cfg.n_layers),\n",
    "            \"transferred_token_num\": n_tokens_to_transfer,\n",
    "            \"orig_prompt\": prompt_original,\n",
    "            \"transplant_prompt\": prompt_new,\n",
    "            \"other_info\": f\"gemma-{topic1}-{topic2}-shorter topic-attempt 2\",\n",
    "            \"output\": output[1],\n",
    "        }\n",
    "\n",
    "        with open(filename, \"a\") as file:\n",
    "            file.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
